{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA512 HW2 Considering Bias in Data\n",
    "\n",
    "The goal of this assignment is to explore the concept of bias in data using Wikipedia articles. This assignment will consider articles about cities in different US states. For this assignment, we will combine a dataset of Wikipedia articles with a dataset of state populations, and use a machine learning service called ORES to estimate the quality of the articles about the cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 aspects of this homework. The first part is about how to access page info data using the [MediaWiki REST API for the EN Wikipedia](https://www.mediawiki.org/wiki/API:Main_page). We request summary 'page info' for multiple article pages. The API documentation, [API:Info](https://www.mediawiki.org/wiki/API:Info), covers additional details that may be helpful when trying to use or understand this part.\n",
    "\n",
    "The next part is to request ORES scores through LiftWing ML Service API. We generate article quality estimates for article revisions using the LiftWing version of [ORES](https://www.mediawiki.org/wiki/ORES). The [ORES API documentation](https://ores.wikimedia.org/docs) can be accessed from the main ORES page. \n",
    "\n",
    "## License\n",
    "Some parts of the code are either used as is or modified based on the example code that was developed by Dr. David W. McDonald for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided under the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "Load all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import json, time, urllib.parse\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import islice\n",
    "from pandas import json_normalize\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard coded variables to change before running this code:\n",
    "1. Working Directory has to be changed under `Set Directory`\n",
    "2. In `Step 2: Getting Article Quality Predictions`, use your own `email_address`, `access_token` & `user_name`. \n",
    "\n",
    "To Get your access token: You will need a Wikimedia user account to get access to Lift Wing (the ML API service). You can either [create an account or login](https://api.wikimedia.org/w/index.php?title=Special:UserLogin&centralAuthAutologinTried=1&centralAuthError=Not+centrally+logged+in). If you have a Wikipedia user account - you might already have an Wikimedia account. If you are not sure try your Wikipedia username and password to check it. If you do not have a Wikimedia account you will need to create an account that you can use to get an access token.\n",
    "\n",
    "There is a 'guide' that describes how to get authentication tokens - but not everything works the way it is described in that documentation. You should review that documentation and then read the rest of this comment.\n",
    "\n",
    "The documentation talks about using a \"dashboard\" for managing authentication tokens. That's a rather generous description for what looks like a simple list of token things. You might have a hard time finding this \"dashboard\". First, on the left hand side of the page, you'll see a column of links. The bottom section is a set of links titled \"Tools\". In that section is a link that says [Special pages](https://api.wikimedia.org/wiki/Special:SpecialPages) which will take you to a list of ... well, special pages. At the very bottom of the \"Special pages\" page is a section titled \"Other special pages\" (scroll all the way to the bottom). The first link in that section is called [API keys](https://api.wikimedia.org/wiki/Special:AppManagement). When you get to the \"API keys\" page you can create a new key.\n",
    "\n",
    "The authentication guide suggests that you should create a server-side app key. This does not seem to work correctly - as yet. It failed on multiple attempts when I attempted to create a server-side app key. BUT, there is an option to create a Personal API token that should work for this course and the type of ORES page scoring that you will need to perform.\n",
    "\n",
    "Note, when you create a Personal API token you are granted the three items - a Client ID, a Client secret, and a Access token - you shold save all three of these. When you dismiss the box they are gone. If you lose any one of the tokens you can destroy or deactivate the Personal API token from the dashboard and then create a new one.\n",
    "\n",
    "\n",
    "Without changing these values, the code would not run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this\n",
    "cwd = 'C:\\\\Users\\\\adith\\\\Documents\\\\HW2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Getting the Article, Population and Region Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 3 flatfiles as a part of our analysis.\n",
    "#### List of Cities\n",
    "The Wikipedia Category:Lists of cities in the United States by state was crawled to generate a list of Wikipedia article pages about US cities from each state. This data is available in the input folder called `us_cities_by_state_SEPT.2023.csv`.\n",
    "\n",
    "#### Population Data\n",
    "We use the data provided by US Census Bureau which has population estimates for every US state. We can find Excel file linked to that page contains estimated populations of all US states for 2022 in the input folder named `NST-EST2022-POP.xlsx`.\n",
    "\n",
    "#### Regional Division Data\n",
    "We also use the regional and divisional agglomerations as defined by the US Census Bureau. The input folder contains a spreadsheet listing the states in each regional division, named `US States by Region - US Census Bureau.xlsx`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>page_title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abbeville,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Adamsville,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Addison,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Akron,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Alabaster,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21520</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Wamsutter, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wamsutter,_Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21521</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Wheatland, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wheatland,_Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21522</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Worland, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Worland,_Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21523</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Wright, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wright,_Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21524</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Yoder, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Yoder,_Wyoming</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21525 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         state           page_title  \\\n",
       "0      Alabama   Abbeville, Alabama   \n",
       "1      Alabama  Adamsville, Alabama   \n",
       "2      Alabama     Addison, Alabama   \n",
       "3      Alabama       Akron, Alabama   \n",
       "4      Alabama   Alabaster, Alabama   \n",
       "...        ...                  ...   \n",
       "21520  Wyoming   Wamsutter, Wyoming   \n",
       "21521  Wyoming   Wheatland, Wyoming   \n",
       "21522  Wyoming     Worland, Wyoming   \n",
       "21523  Wyoming      Wright, Wyoming   \n",
       "21524  Wyoming       Yoder, Wyoming   \n",
       "\n",
       "                                                     url  \n",
       "0       https://en.wikipedia.org/wiki/Abbeville,_Alabama  \n",
       "1      https://en.wikipedia.org/wiki/Adamsville,_Alabama  \n",
       "2         https://en.wikipedia.org/wiki/Addison,_Alabama  \n",
       "3           https://en.wikipedia.org/wiki/Akron,_Alabama  \n",
       "4       https://en.wikipedia.org/wiki/Alabaster,_Alabama  \n",
       "...                                                  ...  \n",
       "21520   https://en.wikipedia.org/wiki/Wamsutter,_Wyoming  \n",
       "21521   https://en.wikipedia.org/wiki/Wheatland,_Wyoming  \n",
       "21522     https://en.wikipedia.org/wiki/Worland,_Wyoming  \n",
       "21523      https://en.wikipedia.org/wiki/Wright,_Wyoming  \n",
       "21524       https://en.wikipedia.org/wiki/Yoder,_Wyoming  \n",
       "\n",
       "[21525 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the wikipedia list of cities data\n",
    "us_citiesxstates = pd.read_csv(os.path.join(cwd, 'input/us_cities_by_state_SEPT.2023.csv'))\n",
    "\n",
    "# Drop duplicate rows\n",
    "us_citiesxstates.drop_duplicates(inplace=True, ignore_index=True)\n",
    "\n",
    "us_citiesxstates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>2022</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>5074296.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>733583.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>7359197.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>3045637.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>39029342.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>5839926.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Connecticut</td>\n",
       "      <td>3626205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Delaware</td>\n",
       "      <td>1018396.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>District of Columbia</td>\n",
       "      <td>671803.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Florida</td>\n",
       "      <td>22244823.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Georgia</td>\n",
       "      <td>10912876.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Hawaii</td>\n",
       "      <td>1440196.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Idaho</td>\n",
       "      <td>1939033.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Illinois</td>\n",
       "      <td>12582032.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Indiana</td>\n",
       "      <td>6833037.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Iowa</td>\n",
       "      <td>3200517.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>2937150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Kentucky</td>\n",
       "      <td>4512310.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Louisiana</td>\n",
       "      <td>4590241.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Maine</td>\n",
       "      <td>1385340.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Maryland</td>\n",
       "      <td>6164660.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>6981974.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Michigan</td>\n",
       "      <td>10034113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Minnesota</td>\n",
       "      <td>5717184.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Mississippi</td>\n",
       "      <td>2940057.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Missouri</td>\n",
       "      <td>6177957.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Montana</td>\n",
       "      <td>1122867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Nebraska</td>\n",
       "      <td>1967923.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>3177772.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>1395231.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>New Jersey</td>\n",
       "      <td>9261699.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>New Mexico</td>\n",
       "      <td>2113344.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>New York</td>\n",
       "      <td>19677151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>North Carolina</td>\n",
       "      <td>10698973.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>North Dakota</td>\n",
       "      <td>779261.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Ohio</td>\n",
       "      <td>11756058.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>4019800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Oregon</td>\n",
       "      <td>4240137.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>12972008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Rhode Island</td>\n",
       "      <td>1093734.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>South Carolina</td>\n",
       "      <td>5282634.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>South Dakota</td>\n",
       "      <td>909824.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Tennessee</td>\n",
       "      <td>7051339.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Texas</td>\n",
       "      <td>30029572.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Utah</td>\n",
       "      <td>3380800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Vermont</td>\n",
       "      <td>647064.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>8683619.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Washington</td>\n",
       "      <td>7785786.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>West Virginia</td>\n",
       "      <td>1775156.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>5892539.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>581381.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>3221789.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   State        2022\n",
       "0                Alabama   5074296.0\n",
       "1                 Alaska    733583.0\n",
       "2                Arizona   7359197.0\n",
       "3               Arkansas   3045637.0\n",
       "4             California  39029342.0\n",
       "5               Colorado   5839926.0\n",
       "6            Connecticut   3626205.0\n",
       "7               Delaware   1018396.0\n",
       "8   District of Columbia    671803.0\n",
       "9                Florida  22244823.0\n",
       "10               Georgia  10912876.0\n",
       "11                Hawaii   1440196.0\n",
       "12                 Idaho   1939033.0\n",
       "13              Illinois  12582032.0\n",
       "14               Indiana   6833037.0\n",
       "15                  Iowa   3200517.0\n",
       "16                Kansas   2937150.0\n",
       "17              Kentucky   4512310.0\n",
       "18             Louisiana   4590241.0\n",
       "19                 Maine   1385340.0\n",
       "20              Maryland   6164660.0\n",
       "21         Massachusetts   6981974.0\n",
       "22              Michigan  10034113.0\n",
       "23             Minnesota   5717184.0\n",
       "24           Mississippi   2940057.0\n",
       "25              Missouri   6177957.0\n",
       "26               Montana   1122867.0\n",
       "27              Nebraska   1967923.0\n",
       "28                Nevada   3177772.0\n",
       "29         New Hampshire   1395231.0\n",
       "30            New Jersey   9261699.0\n",
       "31            New Mexico   2113344.0\n",
       "32              New York  19677151.0\n",
       "33        North Carolina  10698973.0\n",
       "34          North Dakota    779261.0\n",
       "35                  Ohio  11756058.0\n",
       "36              Oklahoma   4019800.0\n",
       "37                Oregon   4240137.0\n",
       "38          Pennsylvania  12972008.0\n",
       "39          Rhode Island   1093734.0\n",
       "40        South Carolina   5282634.0\n",
       "41          South Dakota    909824.0\n",
       "42             Tennessee   7051339.0\n",
       "43                 Texas  30029572.0\n",
       "44                  Utah   3380800.0\n",
       "45               Vermont    647064.0\n",
       "46              Virginia   8683619.0\n",
       "47            Washington   7785786.0\n",
       "48         West Virginia   1775156.0\n",
       "49             Wisconsin   5892539.0\n",
       "50               Wyoming    581381.0\n",
       "51           Puerto Rico   3221789.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the population data\n",
    "\n",
    "us_pop = pd.read_excel(os.path.join(cwd, 'input/NST-EST2022-POP.xlsx'), skiprows=4)\n",
    "\n",
    "# Remove unnecessary rows\n",
    "us_pop = us_pop[4:]  \n",
    "\n",
    "us_pop.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Rename the columns\n",
    "us_pop.columns = ['State','2020_est','2020', '2021', '2022']\n",
    "\n",
    "# The State names start with a . so we remove such special characters\n",
    "us_pop = us_pop[us_pop['State'].str.contains('^\\.', na=False)]\n",
    "us_pop['State'] = us_pop['State'].str.slice(1)\n",
    "us_pop = us_pop[['State', '2022']].reset_index(drop=True)\n",
    "\n",
    "us_pop.columns = ['State', '2022']\n",
    "\n",
    "us_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>division</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Northeast</td>\n",
       "      <td>New England</td>\n",
       "      <td>Connecticut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Northeast</td>\n",
       "      <td>New England</td>\n",
       "      <td>Maine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Northeast</td>\n",
       "      <td>New England</td>\n",
       "      <td>Massachusetts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Northeast</td>\n",
       "      <td>New England</td>\n",
       "      <td>New Hampshire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Northeast</td>\n",
       "      <td>New England</td>\n",
       "      <td>Rhode Island</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Northeast</td>\n",
       "      <td>New England</td>\n",
       "      <td>Vermont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Northeast</td>\n",
       "      <td>Middle Atlantic</td>\n",
       "      <td>New Jersey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Northeast</td>\n",
       "      <td>Middle Atlantic</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Northeast</td>\n",
       "      <td>Middle Atlantic</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Midwest</td>\n",
       "      <td>East North Central</td>\n",
       "      <td>Illinois</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Midwest</td>\n",
       "      <td>East North Central</td>\n",
       "      <td>Indiana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Midwest</td>\n",
       "      <td>East North Central</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Midwest</td>\n",
       "      <td>East North Central</td>\n",
       "      <td>Ohio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Midwest</td>\n",
       "      <td>East North Central</td>\n",
       "      <td>Wisconsin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Midwest</td>\n",
       "      <td>West North Central</td>\n",
       "      <td>Iowa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Midwest</td>\n",
       "      <td>West North Central</td>\n",
       "      <td>Kansas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Midwest</td>\n",
       "      <td>West North Central</td>\n",
       "      <td>Minnesota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Midwest</td>\n",
       "      <td>West North Central</td>\n",
       "      <td>Missouri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Midwest</td>\n",
       "      <td>West North Central</td>\n",
       "      <td>Nebraska</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Midwest</td>\n",
       "      <td>West North Central</td>\n",
       "      <td>North Dakota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Midwest</td>\n",
       "      <td>West North Central</td>\n",
       "      <td>South Dakota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>South</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>Delaware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>South</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>South</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>Georgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>South</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>South</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>North Carolina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>South</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>South Carolina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>South</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>Virginia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>South</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>West Virginia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>South</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>South</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>Kentucky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>South</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>Mississippi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>South</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>Tennessee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>South</td>\n",
       "      <td>West South Central</td>\n",
       "      <td>Arkansas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>South</td>\n",
       "      <td>West South Central</td>\n",
       "      <td>Louisiana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>South</td>\n",
       "      <td>West South Central</td>\n",
       "      <td>Oklahoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>South</td>\n",
       "      <td>West South Central</td>\n",
       "      <td>Texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>West</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>West</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>Colorado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>West</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>Idaho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>West</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>Montana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>West</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>Nevada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>West</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>New Mexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>West</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>Utah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>West</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>West</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>Alaska</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>West</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>West</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>Hawaii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>West</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>Oregon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>West</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>Washington</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       region            division           state\n",
       "2   Northeast         New England     Connecticut\n",
       "3   Northeast         New England           Maine\n",
       "4   Northeast         New England   Massachusetts\n",
       "5   Northeast         New England   New Hampshire\n",
       "6   Northeast         New England    Rhode Island\n",
       "7   Northeast         New England         Vermont\n",
       "9   Northeast     Middle Atlantic      New Jersey\n",
       "10  Northeast     Middle Atlantic        New York\n",
       "11  Northeast     Middle Atlantic    Pennsylvania\n",
       "14    Midwest  East North Central        Illinois\n",
       "15    Midwest  East North Central         Indiana\n",
       "16    Midwest  East North Central        Michigan\n",
       "17    Midwest  East North Central            Ohio\n",
       "18    Midwest  East North Central       Wisconsin\n",
       "20    Midwest  West North Central            Iowa\n",
       "21    Midwest  West North Central          Kansas\n",
       "22    Midwest  West North Central       Minnesota\n",
       "23    Midwest  West North Central        Missouri\n",
       "24    Midwest  West North Central        Nebraska\n",
       "25    Midwest  West North Central    North Dakota\n",
       "26    Midwest  West North Central    South Dakota\n",
       "29      South      South Atlantic        Delaware\n",
       "30      South      South Atlantic         Florida\n",
       "31      South      South Atlantic         Georgia\n",
       "32      South      South Atlantic        Maryland\n",
       "33      South      South Atlantic  North Carolina\n",
       "34      South      South Atlantic  South Carolina\n",
       "35      South      South Atlantic        Virginia\n",
       "36      South      South Atlantic   West Virginia\n",
       "38      South  East South Central         Alabama\n",
       "39      South  East South Central        Kentucky\n",
       "40      South  East South Central     Mississippi\n",
       "41      South  East South Central       Tennessee\n",
       "43      South  West South Central        Arkansas\n",
       "44      South  West South Central       Louisiana\n",
       "45      South  West South Central        Oklahoma\n",
       "46      South  West South Central           Texas\n",
       "49       West            Mountain         Arizona\n",
       "50       West            Mountain        Colorado\n",
       "51       West            Mountain           Idaho\n",
       "52       West            Mountain         Montana\n",
       "53       West            Mountain          Nevada\n",
       "54       West            Mountain      New Mexico\n",
       "55       West            Mountain            Utah\n",
       "56       West            Mountain         Wyoming\n",
       "58       West             Pacific          Alaska\n",
       "59       West             Pacific      California\n",
       "60       West             Pacific          Hawaii\n",
       "61       West             Pacific          Oregon\n",
       "62       West             Pacific      Washington"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the regional divison data from the Excel file\n",
    "us_regions = pd.read_excel(os.path.join(cwd, 'input/US States by Region - US Census Bureau.xlsx'))\n",
    "\n",
    "# Forward-fill missing region and division values\n",
    "us_regions['REGION'].fillna(method='ffill', inplace=True)\n",
    "us_regions['DIVISION'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Filter out rows where STATE is not null (i.e., where STATE is a state name)\n",
    "us_regions = us_regions.dropna(subset=['STATE'])\n",
    "\n",
    "# Rename columns to lowercase\n",
    "us_regions.columns = us_regions.columns.str.lower()\n",
    "\n",
    "us_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Global Variables for the English Wikipedia endpoint\n",
    "In this section, we define various constants and configuration settings that will be used throughout the script to interact with the English Wikipedia API and manage the data retrieval process. These constants include the API endpoint, assumed latency, request headers, example article titles, and parameters for making page information requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The basic English Wikipedia API endpoint\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# We'll assume that there needs to be some throttling for these requests - we should always be nice to a free data resource\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "# This should include an email - your UW email would be good to put in there\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<adi279@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2023',\n",
    "}\n",
    "\n",
    "# This is just a list of English Wikipedia article titles that we can use for example requests. We use another list in this code.\n",
    "ARTICLE_TITLES = ['Abbeville, Alabama', 'Bison', 'Northern flicker', 'Red squirrel', 'Chinook salmon', 'Horseshoe bat' ]\n",
    "\n",
    "# This is a string of additional page properties that can be returned see the Info documentation for\n",
    "# what can be included. If you don't want any this can simply be the empty string\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\n",
    "#PAGEINFO_EXTENDED_PROPERTIES = \"\"\n",
    "\n",
    "# This template lists the basic parameters for making this\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",           # to simplify this should be a single page title at a time\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Request Page Information per Article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Request Page Information per Article\n",
    "\n",
    "This function is used to make requests to the Wikipedia API to retrieve page information for a given article. It takes the following parameters:\n",
    "\n",
    "- `article_title`: The title of the article for which page information is requested.\n",
    "- `endpoint_url`: The URL of the Wikipedia API endpoint (default is `API_ENWIKIPEDIA_ENDPOINT`).\n",
    "- `request_template`: A template containing request parameters (default is `PAGEINFO_PARAMS_TEMPLATE`).\n",
    "- `headers`: Request headers (default is `REQUEST_HEADERS`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_pageinfo_per_article(article_title = None, \n",
    "                                 endpoint_url = API_ENWIKIPEDIA_ENDPOINT, \n",
    "                                 request_template = PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                 headers = REQUEST_HEADERS):\n",
    "    \n",
    "    # article title can be as a parameter to the call or in the request_template\n",
    "    if article_title:\n",
    "        request_template['titles'] = article_title\n",
    "\n",
    "    if not request_template['titles']:\n",
    "        raise Exception(\"Must supply an article title to make a pageinfo request.\")\n",
    "\n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or any other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection:\n",
    "\n",
    "This code block is responsible for collecting and processing data for a set of articles. It operates in the following steps:\n",
    "\n",
    "#### Iteration Over DataFrame Rows:\n",
    "\n",
    "- It iterates over each row of a DataFrame named `us_citiesxstates`.\n",
    "- For each row, it extracts the value in the 'page_title' column and assigns it to the variable `article_title`.\n",
    "\n",
    "#### API Requests for Page Information:\n",
    "\n",
    "- For each `article_title`, the code makes an API request to gather page information.\n",
    "\n",
    "#### Data Storage:\n",
    "\n",
    "- The retrieved data is stored in dictionaries for further processing.\n",
    "\n",
    "#### Handling Failed Requests:\n",
    "\n",
    "- If a request fails, the code logs the failed article titles and continues processing others.\n",
    "- This is in `failed_articles`.\n",
    "\n",
    "\n",
    "#### Output:\n",
    "\n",
    "The resulting DataFrame contains information about articles, including page ID, revision ID, quality score, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling data for:  Abbeville, Alabama\n",
      "Pulling data for:  Adamsville, Alabama\n",
      "Pulling data for:  Addison, Alabama\n",
      "Pulling data for:  Akron, Alabama\n",
      "Pulling data for:  Alabaster, Alabama\n",
      "Pulling data for:  Albertville, Alabama\n",
      "Pulling data for:  Alexander City, Alabama\n",
      "Pulling data for:  Aliceville, Alabama\n",
      "Pulling data for:  Allgood, Alabama\n",
      "Pulling data for:  Altoona, Alabama\n",
      "Pulling data for:  Andalusia, Alabama\n",
      "Pulling data for:  Anderson, Lauderdale County, Alabama\n",
      "Pulling data for:  Anniston, Alabama\n",
      "Pulling data for:  Arab, Alabama\n",
      "Pulling data for:  Ardmore, Alabama\n",
      "Pulling data for:  Argo, Alabama\n",
      "Pulling data for:  Ariton, Alabama\n",
      "Pulling data for:  Arley, Alabama\n",
      "Pulling data for:  Ashford, Alabama\n",
      "Pulling data for:  Ashland, Alabama\n",
      "Pulling data for:  Ashville, Alabama\n",
      "Pulling data for:  Athens, Alabama\n",
      "Pulling data for:  Atmore, Alabama\n",
      "Pulling data for:  Attalla, Alabama\n",
      "Pulling data for:  Auburn, Alabama\n",
      "Pulling data for:  Autaugaville, Alabama\n",
      "Pulling data for:  Avon, Alabama\n",
      "Pulling data for:  Babbie, Alabama\n",
      "Pulling data for:  Baileyton, Alabama\n",
      "Pulling data for:  Bakerhill, Alabama\n",
      "Pulling data for:  Banks, Alabama\n",
      "Pulling data for:  Bay Minette, Alabama\n",
      "Pulling data for:  Bayou La Batre, Alabama\n",
      "Pulling data for:  Bear Creek, Alabama\n",
      "Pulling data for:  Beatrice, Alabama\n",
      "Pulling data for:  Beaverton, Alabama\n",
      "Pulling data for:  Belk, Alabama\n",
      "Pulling data for:  Benton, Alabama\n",
      "Pulling data for:  Berlin, Alabama\n",
      "Pulling data for:  Berry, Alabama\n",
      "Pulling data for:  Bessemer, Alabama\n",
      "Pulling data for:  Billingsley, Alabama\n",
      "Pulling data for:  Birmingham, Alabama\n",
      "Pulling data for:  Black, Alabama\n",
      "Pulling data for:  Blountsville, Alabama\n",
      "Pulling data for:  Blue Springs, Alabama\n",
      "Pulling data for:  Boaz, Alabama\n",
      "Pulling data for:  Boligee, Alabama\n",
      "Pulling data for:  Bon Air, Alabama\n",
      "Pulling data for:  Brantley, Alabama\n",
      "Pulling data for:  Brent, Alabama\n",
      "Pulling data for:  Brewton, Alabama\n",
      "Pulling data for:  Bridgeport, Alabama\n",
      "Pulling data for:  Brighton, Alabama\n",
      "Pulling data for:  Brilliant, Alabama\n",
      "Pulling data for:  Brookside, Alabama\n",
      "Pulling data for:  Brookwood, Alabama\n",
      "Pulling data for:  Brundidge, Alabama\n",
      "Pulling data for:  Butler, Alabama\n",
      "Pulling data for:  Calera, Alabama\n",
      "Pulling data for:  Camden, Alabama\n",
      "Pulling data for:  Camp Hill, Alabama\n",
      "Pulling data for:  Carbon Hill, Alabama\n",
      "Pulling data for:  Cardiff, Alabama\n",
      "Pulling data for:  Carolina, Alabama\n",
      "Pulling data for:  Carrollton, Alabama\n",
      "Pulling data for:  Castleberry, Alabama\n",
      "Pulling data for:  Cedar Bluff, Alabama\n",
      "Pulling data for:  Center Point, Alabama\n",
      "Pulling data for:  Centre, Alabama\n",
      "Pulling data for:  Centreville, Alabama\n",
      "Pulling data for:  Chatom, Alabama\n",
      "Pulling data for:  Chelsea, Alabama\n",
      "Pulling data for:  Cherokee, Alabama\n",
      "Pulling data for:  Chickasaw, Alabama\n",
      "Pulling data for:  Childersburg, Alabama\n",
      "Pulling data for:  Citronelle, Alabama\n",
      "Pulling data for:  Clanton, Alabama\n",
      "Pulling data for:  Clay, Alabama\n",
      "Pulling data for:  Clayhatchee, Alabama\n",
      "Pulling data for:  Clayton, Alabama\n",
      "Pulling data for:  Cleveland, Alabama\n",
      "Pulling data for:  Clio, Alabama\n",
      "Pulling data for:  Coaling, Alabama\n",
      "Pulling data for:  Coffee Springs, Alabama\n",
      "Pulling data for:  Coffeeville, Alabama\n",
      "Pulling data for:  Coker, Alabama\n",
      "Pulling data for:  Collinsville, Alabama\n",
      "Pulling data for:  Colony, Alabama\n",
      "Pulling data for:  Columbia, Alabama\n",
      "Pulling data for:  Columbiana, Alabama\n",
      "Pulling data for:  Coosada, Alabama\n",
      "Pulling data for:  Cordova, Alabama\n",
      "Pulling data for:  Cottonwood, Alabama\n",
      "Pulling data for:  County Line, Alabama\n",
      "Pulling data for:  Courtland, Alabama\n",
      "Pulling data for:  Cowarts, Alabama\n",
      "Pulling data for:  Creola, Alabama\n",
      "Pulling data for:  Crossville, Alabama\n",
      "Pulling data for:  Cuba, Alabama\n",
      "Pulling data for:  Cullman, Alabama\n",
      "Pulling data for:  Cusseta, Alabama\n",
      "Pulling data for:  Dadeville, Alabama\n",
      "Pulling data for:  Daleville, Alabama\n",
      "Pulling data for:  Daphne, Alabama\n",
      "Pulling data for:  Dauphin Island, Alabama\n",
      "Pulling data for:  Daviston, Alabama\n",
      "Pulling data for:  Dayton, Alabama\n",
      "Pulling data for:  Deatsville, Alabama\n",
      "Pulling data for:  Decatur, Alabama\n",
      "Pulling data for:  Demopolis, Alabama\n",
      "Pulling data for:  Detroit, Alabama\n",
      "Pulling data for:  Dodge City, Alabama\n",
      "Pulling data for:  Dora, Alabama\n",
      "Pulling data for:  Dothan, Alabama\n",
      "Pulling data for:  Double Springs, Alabama\n",
      "Pulling data for:  Douglas, Alabama\n",
      "Pulling data for:  Dozier, Alabama\n",
      "Pulling data for:  Dutton, Alabama\n",
      "Pulling data for:  East Brewton, Alabama\n",
      "Pulling data for:  Eclectic, Alabama\n",
      "Pulling data for:  Edwardsville, Alabama\n",
      "Pulling data for:  Elba, Alabama\n",
      "Pulling data for:  Elberta, Alabama\n",
      "Pulling data for:  Eldridge, Alabama\n",
      "Pulling data for:  Elkmont, Alabama\n",
      "Pulling data for:  Elmore, Alabama\n",
      "Pulling data for:  Emelle, Alabama\n",
      "Pulling data for:  Enterprise, Alabama\n",
      "Pulling data for:  Epes, Alabama\n",
      "Pulling data for:  Ethelsville, Alabama\n",
      "Pulling data for:  Eufaula, Alabama\n",
      "Pulling data for:  Eutaw, Alabama\n",
      "Pulling data for:  Eva, Alabama\n",
      "Pulling data for:  Evergreen, Conecuh County, Alabama\n",
      "Pulling data for:  Excel, Alabama\n",
      "Pulling data for:  Fairfield, Alabama\n",
      "Pulling data for:  Fairhope, Alabama\n",
      "Pulling data for:  Fairview, Alabama\n",
      "Pulling data for:  Falkville, Alabama\n",
      "Pulling data for:  Faunsdale, Alabama\n",
      "Pulling data for:  Fayette, Alabama\n",
      "Pulling data for:  Five Points, Alabama\n",
      "Pulling data for:  Flomaton, Alabama\n",
      "Pulling data for:  Florala, Alabama\n",
      "Pulling data for:  Florence, Alabama\n",
      "Pulling data for:  Foley, Alabama\n",
      "Pulling data for:  Forkland, Alabama\n",
      "Pulling data for:  Fort Deposit, Alabama\n",
      "Pulling data for:  Fort Payne, Alabama\n",
      "Pulling data for:  Franklin, Alabama\n",
      "Pulling data for:  Frisco City, Alabama\n",
      "Pulling data for:  Fruithurst, Alabama\n",
      "Pulling data for:  Fulton, Alabama\n",
      "Pulling data for:  Fultondale, Alabama\n",
      "Pulling data for:  Fyffe, Alabama\n",
      "Pulling data for:  Gadsden, Alabama\n",
      "Pulling data for:  Gainesville, Alabama\n",
      "Pulling data for:  Gantt, Alabama\n",
      "Pulling data for:  Garden City, Alabama\n",
      "Pulling data for:  Gardendale, Alabama\n",
      "Pulling data for:  Gaylesville, Alabama\n",
      "Pulling data for:  Geiger, Alabama\n",
      "Pulling data for:  Geneva, Alabama\n",
      "Pulling data for:  Georgiana, Alabama\n",
      "Pulling data for:  Geraldine, Alabama\n",
      "Pulling data for:  Gilbertown, Alabama\n",
      "Pulling data for:  Glen Allen, Alabama\n",
      "Pulling data for:  Glencoe, Alabama\n",
      "Pulling data for:  Glenwood, Alabama\n",
      "Pulling data for:  Goldville, Alabama\n",
      "Pulling data for:  Good Hope, Alabama\n",
      "Pulling data for:  Goodwater, Alabama\n",
      "Pulling data for:  Gordo, Alabama\n",
      "Pulling data for:  Gordon, Alabama\n",
      "Pulling data for:  Gordonville, Alabama\n",
      "Pulling data for:  Goshen, Alabama\n",
      "Pulling data for:  Grant, Alabama\n",
      "Pulling data for:  Graysville, Alabama\n",
      "Pulling data for:  Greensboro, Alabama\n",
      "Pulling data for:  Greenville, Alabama\n",
      "Pulling data for:  Grimes, Alabama\n",
      "Pulling data for:  Grove Hill, Alabama\n",
      "Pulling data for:  Guin, Alabama\n",
      "Pulling data for:  Gulf Shores, Alabama\n",
      "Pulling data for:  Guntersville, Alabama\n",
      "Pulling data for:  Gurley, Alabama\n",
      "Pulling data for:  Gu-Win, Alabama\n",
      "Pulling data for:  Hackleburg, Alabama\n",
      "Pulling data for:  Haleburg, Alabama\n",
      "Pulling data for:  Haleyville, Alabama\n",
      "Pulling data for:  Hamilton, Alabama\n",
      "Pulling data for:  Hammondville, Alabama\n",
      "Pulling data for:  Hanceville, Alabama\n",
      "Pulling data for:  Harpersville, Alabama\n",
      "Pulling data for:  Hartford, Alabama\n",
      "Pulling data for:  Hartselle, Alabama\n",
      "Pulling data for:  Hayden, Alabama\n",
      "Pulling data for:  Hayneville, Alabama\n",
      "Pulling data for:  Headland, Alabama\n",
      "Pulling data for:  Heath, Alabama\n",
      "Pulling data for:  Heflin, Alabama\n",
      "Pulling data for:  Helena, Alabama\n",
      "Pulling data for:  Henagar, Alabama\n",
      "Pulling data for:  Highland Lake, Alabama\n",
      "Pulling data for:  Hillsboro, Alabama\n",
      "Pulling data for:  Hobson City, Alabama\n",
      "Pulling data for:  Hodges, Alabama\n",
      "Pulling data for:  Hokes Bluff, Alabama\n",
      "Pulling data for:  Holly Pond, Alabama\n",
      "Pulling data for:  Hollywood, Alabama\n",
      "Pulling data for:  Homewood, Alabama\n",
      "Pulling data for:  Hoover, Alabama\n",
      "Pulling data for:  Horn Hill, Alabama\n",
      "Pulling data for:  Hueytown, Alabama\n",
      "Pulling data for:  Huntsville, Alabama\n",
      "Pulling data for:  Hurtsboro, Alabama\n",
      "Pulling data for:  Hytop, Alabama\n",
      "Pulling data for:  Ider, Alabama\n",
      "Pulling data for:  Indian Springs Village, Alabama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling data for:  Irondale, Alabama\n",
      "Pulling data for:  Jackson, Alabama\n",
      "Pulling data for:  Jackson's Gap, Alabama\n",
      "Pulling data for:  Jacksonville, Alabama\n",
      "Pulling data for:  Jasper, Alabama\n",
      "Pulling data for:  Jemison, Alabama\n",
      "Pulling data for:  Kansas, Alabama\n",
      "Pulling data for:  Kellyton, Alabama\n",
      "Pulling data for:  Kennedy, Alabama\n",
      "Pulling data for:  Killen, Alabama\n",
      "Pulling data for:  Kimberly, Alabama\n",
      "Pulling data for:  Kinsey, Alabama\n",
      "Pulling data for:  Kinston, Alabama\n",
      "Pulling data for:  LaFayette, Alabama\n",
      "Pulling data for:  Lake View, Alabama\n",
      "Pulling data for:  Lakeview, Alabama\n",
      "Pulling data for:  Lanett, Alabama\n",
      "Pulling data for:  Langston, Alabama\n",
      "Pulling data for:  Leeds, Alabama\n",
      "Pulling data for:  Leesburg, Alabama\n",
      "Pulling data for:  Leighton, Alabama\n",
      "Pulling data for:  Lester, Alabama\n",
      "Pulling data for:  Level Plains, Alabama\n",
      "Pulling data for:  Lexington, Alabama\n",
      "Pulling data for:  Libertyville, Alabama\n",
      "Pulling data for:  Lincoln, Alabama\n",
      "Pulling data for:  Linden, Alabama\n",
      "Pulling data for:  Lineville, Alabama\n",
      "Pulling data for:  Lipscomb, Alabama\n",
      "Pulling data for:  Lisman, Alabama\n",
      "Pulling data for:  Littleville, Alabama\n",
      "Pulling data for:  Livingston, Alabama\n",
      "Pulling data for:  Loachapoka, Alabama\n",
      "Pulling data for:  Lockhart, Alabama\n",
      "Pulling data for:  Locust Fork, Alabama\n",
      "Pulling data for:  Louisville, Alabama\n",
      "Pulling data for:  Lowndesboro, Alabama\n",
      "Pulling data for:  Loxley, Alabama\n",
      "Pulling data for:  Luverne, Alabama\n",
      "Pulling data for:  Lynn, Alabama\n",
      "Pulling data for:  Madison, Alabama\n",
      "Pulling data for:  Madrid, Alabama\n",
      "Pulling data for:  Magnolia Springs, Alabama\n",
      "Pulling data for:  Malvern, Alabama\n",
      "Pulling data for:  Maplesville, Alabama\n",
      "Pulling data for:  Margaret, Alabama\n",
      "Pulling data for:  Marion, Alabama\n",
      "Pulling data for:  Maytown, Alabama\n",
      "Pulling data for:  McIntosh, Alabama\n",
      "Pulling data for:  McKenzie, Alabama\n",
      "Pulling data for:  McMullen, Alabama\n",
      "Pulling data for:  Memphis, Alabama\n",
      "Pulling data for:  Mentone, Alabama\n",
      "Pulling data for:  Midfield, Alabama\n",
      "Pulling data for:  Midland City, Alabama\n",
      "Pulling data for:  Midway, Alabama\n",
      "Pulling data for:  Millbrook, Alabama\n",
      "Pulling data for:  Millport, Alabama\n",
      "Pulling data for:  Millry, Alabama\n",
      "Pulling data for:  Mobile, Alabama\n",
      "Pulling data for:  Monroeville, Alabama\n",
      "Pulling data for:  Montevallo, Alabama\n",
      "Pulling data for:  Montgomery, Alabama\n",
      "Pulling data for:  Moody, Alabama\n",
      "Pulling data for:  Mooresville, Alabama\n",
      "Pulling data for:  Morris, Alabama\n",
      "Pulling data for:  Mosses, Alabama\n",
      "Pulling data for:  Moulton, Alabama\n",
      "Pulling data for:  Moundville, Alabama\n",
      "Pulling data for:  Mount Vernon, Alabama\n",
      "Pulling data for:  Mountain Brook, Alabama\n",
      "Pulling data for:  Mulga, Alabama\n",
      "Pulling data for:  Munford, Alabama\n",
      "Pulling data for:  Muscle Shoals, Alabama\n",
      "Pulling data for:  Myrtlewood, Alabama\n",
      "Pulling data for:  Napier Field, Alabama\n",
      "Pulling data for:  Natural Bridge, Alabama\n",
      "Pulling data for:  Nauvoo, Alabama\n",
      "Pulling data for:  Nectar, Alabama\n",
      "Pulling data for:  Needham, Alabama\n",
      "Pulling data for:  New Brockton, Alabama\n",
      "Pulling data for:  New Hope, Alabama\n",
      "Pulling data for:  New Site, Alabama\n",
      "Pulling data for:  Newbern, Alabama\n",
      "Pulling data for:  Newton, Alabama\n",
      "Pulling data for:  Newville, Alabama\n",
      "Pulling data for:  North Courtland, Alabama\n",
      "Pulling data for:  North Johns, Alabama\n",
      "Pulling data for:  Northport, Alabama\n",
      "Pulling data for:  Notasulga, Alabama\n",
      "Pulling data for:  Oak Grove, Alabama\n",
      "Pulling data for:  Oak Hill, Alabama\n",
      "Pulling data for:  Oakman, Alabama\n",
      "Pulling data for:  Odenville, Alabama\n",
      "Pulling data for:  Ohatchee, Alabama\n",
      "Pulling data for:  Oneonta, Alabama\n",
      "Pulling data for:  Onycha, Alabama\n",
      "Pulling data for:  Opelika, Alabama\n",
      "Pulling data for:  Opp, Alabama\n",
      "Pulling data for:  Orange Beach, Alabama\n",
      "Pulling data for:  Orrville, Alabama\n",
      "Pulling data for:  Owens Cross Roads, Alabama\n",
      "Pulling data for:  Oxford, Alabama\n",
      "Pulling data for:  Ozark, Alabama\n",
      "Pulling data for:  Paint Rock, Alabama\n",
      "Pulling data for:  Parrish, Alabama\n",
      "Pulling data for:  Pelham, Alabama\n",
      "Pulling data for:  Pell City, Alabama\n",
      "Pulling data for:  Pennington, Alabama\n",
      "Pulling data for:  Perdido Beach, Alabama\n",
      "Pulling data for:  Petrey, Alabama\n",
      "Pulling data for:  Phenix City, Alabama\n",
      "Pulling data for:  Phil Campbell, Alabama\n",
      "Pulling data for:  Pickensville, Alabama\n",
      "Pulling data for:  Piedmont, Alabama\n",
      "Pulling data for:  Pike Road, Alabama\n",
      "Pulling data for:  Pinckard, Alabama\n",
      "Pulling data for:  Pine Apple, Alabama\n",
      "Pulling data for:  Pine Hill, Alabama\n",
      "Pulling data for:  Pine Ridge, Alabama\n",
      "Pulling data for:  Pinson, Alabama\n",
      "Pulling data for:  Pisgah, Alabama\n",
      "Pulling data for:  Pleasant Grove, Alabama\n",
      "Pulling data for:  Pleasant Groves, Alabama\n",
      "Pulling data for:  Pollard, Alabama\n",
      "Pulling data for:  Powell, Alabama\n",
      "Pulling data for:  Prattville, Alabama\n",
      "Pulling data for:  Priceville, Alabama\n",
      "Pulling data for:  Prichard, Alabama\n",
      "Pulling data for:  Providence, Alabama\n",
      "Pulling data for:  Ragland, Alabama\n",
      "Pulling data for:  Rainbow City, Alabama\n",
      "Pulling data for:  Rainsville, Alabama\n",
      "Pulling data for:  Ranburne, Alabama\n",
      "Pulling data for:  Red Bay, Alabama\n",
      "Pulling data for:  Red Level, Alabama\n",
      "Pulling data for:  Reece City, Alabama\n",
      "Pulling data for:  Reform, Alabama\n",
      "Pulling data for:  Rehobeth, Alabama\n",
      "Pulling data for:  Repton, Alabama\n",
      "Pulling data for:  Ridgeville, Alabama\n",
      "Pulling data for:  River Falls, Alabama\n",
      "Pulling data for:  Riverside, Alabama\n",
      "Pulling data for:  Riverview, Alabama\n",
      "Pulling data for:  Roanoke, Alabama\n",
      "Pulling data for:  Robertsdale, Alabama\n",
      "Pulling data for:  Rockford, Alabama\n",
      "Pulling data for:  Rogersville, Alabama\n",
      "Pulling data for:  Rosa, Alabama\n",
      "Pulling data for:  Russellville, Alabama\n",
      "Pulling data for:  Rutledge, Alabama\n",
      "Pulling data for:  St. Florian, Alabama\n",
      "Pulling data for:  Samson, Alabama\n",
      "Pulling data for:  Sand Rock, Alabama\n",
      "Pulling data for:  Sanford, Alabama\n",
      "Pulling data for:  Saraland, Alabama\n",
      "Pulling data for:  Sardis City, Alabama\n",
      "Pulling data for:  Satsuma, Alabama\n",
      "Pulling data for:  Scottsboro, Alabama\n",
      "Pulling data for:  Section, Alabama\n",
      "Pulling data for:  Selma, Alabama\n",
      "Pulling data for:  Sheffield, Alabama\n",
      "Pulling data for:  Shiloh, DeKalb County, Alabama\n",
      "Pulling data for:  Shorter, Alabama\n",
      "Pulling data for:  Silas, Alabama\n",
      "Pulling data for:  Silverhill, Alabama\n",
      "Pulling data for:  Sipsey, Alabama\n",
      "Pulling data for:  Skyline, Alabama\n",
      "Pulling data for:  Slocomb, Alabama\n",
      "Pulling data for:  Smiths Station, Alabama\n",
      "Pulling data for:  Snead, Alabama\n",
      "Pulling data for:  Somerville, Alabama\n",
      "Pulling data for:  South Vinemont, Alabama\n",
      "Pulling data for:  Southside, Alabama\n",
      "Pulling data for:  Spanish Fort, Alabama\n",
      "Pulling data for:  Springville, Alabama\n",
      "Pulling data for:  Steele, Alabama\n",
      "Pulling data for:  Stevenson, Alabama\n",
      "Pulling data for:  Sulligent, Alabama\n",
      "Pulling data for:  Sumiton, Alabama\n",
      "Pulling data for:  Summerdale, Alabama\n",
      "Pulling data for:  Susan Moore, Alabama\n",
      "Pulling data for:  Sweet Water, Alabama\n",
      "Pulling data for:  Sylacauga, Alabama\n",
      "Pulling data for:  Sylvan Springs, Alabama\n",
      "Pulling data for:  Sylvania, Alabama\n",
      "Pulling data for:  Talladega Springs, Alabama\n",
      "Pulling data for:  Talladega, Alabama\n",
      "Pulling data for:  Tallassee, Alabama\n",
      "Pulling data for:  Tarrant, Alabama\n",
      "Pulling data for:  Taylor, Alabama\n",
      "Pulling data for:  Thomaston, Alabama\n",
      "Pulling data for:  Thomasville, Alabama\n",
      "Pulling data for:  Thorsby, Alabama\n",
      "Pulling data for:  Town Creek, Alabama\n",
      "Pulling data for:  Toxey, Alabama\n",
      "Pulling data for:  Trafford, Alabama\n",
      "Pulling data for:  Triana, Alabama\n",
      "Pulling data for:  Trinity, Alabama\n",
      "Pulling data for:  Troy, Alabama\n",
      "Pulling data for:  Trussville, Alabama\n",
      "Pulling data for:  Tuscaloosa, Alabama\n",
      "Pulling data for:  Tuscumbia, Alabama\n",
      "Pulling data for:  Tuskegee, Alabama\n",
      "Pulling data for:  Twin, Alabama\n",
      "Pulling data for:  Union Grove, Alabama\n",
      "Pulling data for:  Union Springs, Alabama\n",
      "Pulling data for:  Union, Alabama\n",
      "Pulling data for:  Uniontown, Alabama\n",
      "Pulling data for:  Valley, Alabama\n",
      "Pulling data for:  Valley Grande, Alabama\n",
      "Pulling data for:  Valley Head, Alabama\n",
      "Pulling data for:  Vance, Alabama\n",
      "Pulling data for:  Vernon, Alabama\n",
      "Pulling data for:  Vestavia Hills, Alabama\n",
      "Pulling data for:  Vina, Alabama\n",
      "Pulling data for:  Vincent, Alabama\n",
      "Pulling data for:  Vredenburgh, Alabama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling data for:  Wadley, Alabama\n",
      "Pulling data for:  Waldo, Alabama\n",
      "Pulling data for:  Walnut Grove, Alabama\n",
      "Pulling data for:  Warrior, Alabama\n",
      "Pulling data for:  Waterloo, Alabama\n",
      "Pulling data for:  Waverly, Alabama\n",
      "Pulling data for:  Weaver, Alabama\n",
      "Pulling data for:  Webb, Alabama\n",
      "Pulling data for:  Wedowee, Alabama\n",
      "Pulling data for:  West Blocton, Alabama\n",
      "Pulling data for:  West Jefferson, Alabama\n",
      "Pulling data for:  West Point, Alabama\n",
      "Pulling data for:  Westover, Alabama\n",
      "Pulling data for:  Wetumpka, Alabama\n",
      "Pulling data for:  White Hall, Alabama\n",
      "Pulling data for:  Wilsonville, Alabama\n",
      "Pulling data for:  Wilton, Alabama\n",
      "Pulling data for:  Winfield, Alabama\n",
      "Pulling data for:  Woodland, Alabama\n",
      "Pulling data for:  Woodstock, Alabama\n",
      "Pulling data for:  Woodville, Alabama\n",
      "Pulling data for:  Yellow Bluff, Alabama\n",
      "Pulling data for:  York, Alabama\n",
      "Pulling data for:  Adak, Alaska\n",
      "Pulling data for:  Akhiok, Alaska\n",
      "Pulling data for:  Akiak, Alaska\n",
      "Pulling data for:  Akutan, Alaska\n",
      "Pulling data for:  Alakanuk, Alaska\n",
      "Pulling data for:  Aleknagik, Alaska\n",
      "Pulling data for:  Allakaket, Alaska\n",
      "Pulling data for:  Ambler, Alaska\n",
      "Pulling data for:  Anaktuvuk Pass, Alaska\n",
      "Pulling data for:  Anchorage, Alaska\n",
      "Pulling data for:  Anderson, Alaska\n",
      "Pulling data for:  Angoon, Alaska\n",
      "Pulling data for:  Aniak, Alaska\n",
      "Pulling data for:  Anvik, Alaska\n",
      "Pulling data for:  Atka, Alaska\n",
      "Pulling data for:  Atqasuk, Alaska\n",
      "Pulling data for:  Bethel, Alaska\n",
      "Pulling data for:  Bettles, Alaska\n",
      "Pulling data for:  Brevig Mission, Alaska\n",
      "Pulling data for:  Buckland, Alaska\n",
      "Pulling data for:  Chefornak, Alaska\n",
      "Pulling data for:  Chevak, Alaska\n",
      "Pulling data for:  Chignik, Alaska\n",
      "Pulling data for:  Chuathbaluk, Alaska\n",
      "Pulling data for:  Clark's Point, Alaska\n",
      "Pulling data for:  Coffman Cove, Alaska\n",
      "Pulling data for:  Cold Bay, Alaska\n",
      "Pulling data for:  Cordova, Alaska\n",
      "Pulling data for:  Craig, Alaska\n",
      "Pulling data for:  Deering, Alaska\n",
      "Pulling data for:  Delta Junction, Alaska\n",
      "Pulling data for:  Dillingham, Alaska\n",
      "Pulling data for:  Diomede, Alaska\n",
      "Pulling data for:  Eagle, Alaska\n",
      "Pulling data for:  Edna Bay, Alaska\n",
      "Pulling data for:  Eek, Alaska\n",
      "Pulling data for:  Egegik, Alaska\n",
      "Pulling data for:  Ekwok, Alaska\n",
      "Pulling data for:  Elim, Alaska\n",
      "Pulling data for:  Emmonak, Alaska\n",
      "Pulling data for:  Fairbanks, Alaska\n",
      "Pulling data for:  False Pass, Alaska\n",
      "Pulling data for:  Fort Yukon, Alaska\n",
      "Pulling data for:  Galena, Alaska\n",
      "Pulling data for:  Gambell, Alaska\n",
      "Pulling data for:  Golovin, Alaska\n",
      "Pulling data for:  Goodnews Bay, Alaska\n",
      "Pulling data for:  Grayling, Alaska\n",
      "Pulling data for:  Gustavus, Alaska\n",
      "Pulling data for:  Holy Cross, Alaska\n",
      "Pulling data for:  Homer, Alaska\n",
      "Pulling data for:  Hoonah, Alaska\n",
      "Pulling data for:  Hooper Bay, Alaska\n",
      "Pulling data for:  Houston, Alaska\n",
      "Pulling data for:  Hughes, Alaska\n",
      "Pulling data for:  Huslia, Alaska\n",
      "Pulling data for:  Hydaburg, Alaska\n",
      "Pulling data for:  Juneau, Alaska\n",
      "Pulling data for:  Kachemak, Alaska\n",
      "Pulling data for:  Kake, Alaska\n",
      "Pulling data for:  Kaktovik, Alaska\n",
      "Pulling data for:  Kaltag, Alaska\n",
      "Pulling data for:  Kasaan, Alaska\n",
      "Pulling data for:  Kenai, Alaska\n",
      "Pulling data for:  Ketchikan, Alaska\n",
      "Pulling data for:  Kiana, Alaska\n",
      "Pulling data for:  King Cove, Alaska\n",
      "Pulling data for:  Kivalina, Alaska\n",
      "Pulling data for:  Klawock, Alaska\n",
      "Pulling data for:  Kobuk, Alaska\n",
      "Pulling data for:  Kodiak, Alaska\n",
      "Pulling data for:  Kotlik, Alaska\n",
      "Pulling data for:  Kotzebue, Alaska\n",
      "Pulling data for:  Koyuk, Alaska\n",
      "Pulling data for:  Koyukuk, Alaska\n",
      "Pulling data for:  Kupreanof, Alaska\n",
      "Pulling data for:  Kwethluk, Alaska\n",
      "Pulling data for:  Larsen Bay, Alaska\n",
      "Pulling data for:  Lower Kalskag, Alaska\n",
      "Pulling data for:  Manokotak, Alaska\n",
      "Pulling data for:  Marshall, Alaska\n",
      "Pulling data for:  McGrath, Alaska\n",
      "Pulling data for:  Mekoryuk, Alaska\n",
      "Pulling data for:  Mountain Village, Alaska\n",
      "Pulling data for:  Napakiak, Alaska\n",
      "Pulling data for:  Napaskiak, Alaska\n",
      "Pulling data for:  Nenana, Alaska\n",
      "Pulling data for:  New Stuyahok, Alaska\n",
      "Pulling data for:  Newhalen, Alaska\n",
      "Pulling data for:  Nightmute, Alaska\n",
      "Pulling data for:  Nikolai, Alaska\n",
      "Pulling data for:  Nome, Alaska\n",
      "Pulling data for:  Nondalton, Alaska\n",
      "Pulling data for:  Noorvik, Alaska\n",
      "Pulling data for:  North Pole, Alaska\n",
      "Pulling data for:  Nuiqsut, Alaska\n",
      "Pulling data for:  Nulato, Alaska\n",
      "Pulling data for:  Nunam Iqua, Alaska\n",
      "Pulling data for:  Nunapitchuk, Alaska\n",
      "Pulling data for:  Old Harbor, Alaska\n",
      "Pulling data for:  Ouzinkie, Alaska\n",
      "Pulling data for:  Palmer, Alaska\n",
      "Pulling data for:  Pelican, Alaska\n",
      "Pulling data for:  Pilot Point, Alaska\n",
      "Pulling data for:  Pilot Station, Alaska\n",
      "Pulling data for:  Platinum, Alaska\n",
      "Pulling data for:  Point Hope, Alaska\n",
      "Pulling data for:  Port Alexander, Alaska\n",
      "Pulling data for:  Port Heiden, Alaska\n",
      "Pulling data for:  Port Lions, Alaska\n",
      "Pulling data for:  Quinhagak, Alaska\n",
      "Pulling data for:  Ruby, Alaska\n",
      "Pulling data for:  Russian Mission, Alaska\n",
      "Pulling data for:  Saint Paul, Alaska\n",
      "Pulling data for:  Sand Point, Alaska\n",
      "Pulling data for:  Savoonga, Alaska\n",
      "Pulling data for:  Saxman, Alaska\n",
      "Pulling data for:  Scammon Bay, Alaska\n",
      "Pulling data for:  Selawik, Alaska\n",
      "Pulling data for:  Seldovia, Alaska\n",
      "Pulling data for:  Seward, Alaska\n",
      "Pulling data for:  Shageluk, Alaska\n",
      "Pulling data for:  Shaktoolik, Alaska\n",
      "Pulling data for:  Shishmaref, Alaska\n",
      "Pulling data for:  Shungnak, Alaska\n",
      "Pulling data for:  Sitka, Alaska\n",
      "Pulling data for:  Soldotna, Alaska\n",
      "Pulling data for:  St. George, Alaska\n",
      "Pulling data for:  St. Mary's, Alaska\n",
      "Pulling data for:  St. Michael, Alaska\n",
      "Pulling data for:  Stebbins, Alaska\n",
      "Pulling data for:  Tanana, Alaska\n",
      "Pulling data for:  Teller, Alaska\n",
      "Pulling data for:  Tenakee Springs, Alaska\n",
      "Pulling data for:  Thorne Bay, Alaska\n",
      "Pulling data for:  Togiak, Alaska\n",
      "Pulling data for:  Toksook Bay, Alaska\n",
      "Pulling data for:  Unalakleet, Alaska\n",
      "Pulling data for:  Unalaska, Alaska\n",
      "Pulling data for:  Upper Kalskag, Alaska\n",
      "Pulling data for:  UtqiaÄ¡vik\n",
      "Pulling data for:  Valdez, Alaska\n",
      "Pulling data for:  Wainwright, Alaska\n",
      "Pulling data for:  Wales, Alaska\n",
      "Pulling data for:  Wasilla, Alaska\n",
      "Pulling data for:  Whale Pass, Alaska\n",
      "Pulling data for:  White Mountain, Alaska\n",
      "Pulling data for:  Whittier, Alaska\n",
      "Pulling data for:  Wrangell, Alaska\n",
      "Pulling data for:  Apache Junction, Arizona\n",
      "Pulling data for:  Avondale, Arizona\n",
      "Pulling data for:  Benson, Arizona\n",
      "Pulling data for:  Bisbee, Arizona\n",
      "Pulling data for:  Buckeye, Arizona\n",
      "Pulling data for:  Bullhead City, Arizona\n",
      "Pulling data for:  Camp Verde, Arizona\n",
      "Pulling data for:  Carefree, Arizona\n",
      "Pulling data for:  Casa Grande, Arizona\n",
      "Pulling data for:  Cave Creek, Arizona\n",
      "Pulling data for:  Chandler, Arizona\n",
      "Pulling data for:  Chino Valley, Arizona\n",
      "Pulling data for:  Clarkdale, Arizona\n",
      "Pulling data for:  Clifton, Arizona\n",
      "Pulling data for:  Colorado City, Arizona\n",
      "Pulling data for:  Coolidge, Arizona\n",
      "Pulling data for:  Cottonwood, Arizona\n",
      "Pulling data for:  Dewey-Humboldt, Arizona\n",
      "Pulling data for:  Douglas, Arizona\n",
      "Pulling data for:  Duncan, Arizona\n",
      "Pulling data for:  Eagar, Arizona\n",
      "Pulling data for:  El Mirage, Arizona\n",
      "Pulling data for:  Eloy, Arizona\n",
      "Pulling data for:  Flagstaff, Arizona\n",
      "Pulling data for:  Florence, Arizona\n",
      "Pulling data for:  Fountain Hills, Arizona\n",
      "Pulling data for:  Fredonia, Arizona\n",
      "Pulling data for:  Gila Bend, Arizona\n",
      "Pulling data for:  Gilbert, Arizona\n",
      "Pulling data for:  Glendale, Arizona\n",
      "Pulling data for:  Globe, Arizona\n",
      "Pulling data for:  Goodyear, Arizona\n",
      "Pulling data for:  Guadalupe, Arizona\n",
      "Pulling data for:  Hayden, Arizona\n",
      "Pulling data for:  Holbrook, Arizona\n",
      "Pulling data for:  Huachuca City, Arizona\n",
      "Pulling data for:  Jerome, Arizona\n",
      "Pulling data for:  Kearny, Arizona\n",
      "Pulling data for:  Kingman, Arizona\n",
      "Pulling data for:  Lake Havasu City, Arizona\n",
      "Pulling data for:  Litchfield Park, Arizona\n",
      "Pulling data for:  Mammoth, Arizona\n",
      "Pulling data for:  Marana, Arizona\n",
      "Pulling data for:  Maricopa, Arizona\n",
      "Pulling data for:  Mesa, Arizona\n",
      "Pulling data for:  Miami, Arizona\n",
      "Pulling data for:  Nogales, Arizona\n",
      "Pulling data for:  Oro Valley, Arizona\n",
      "Pulling data for:  Page, Arizona\n",
      "Pulling data for:  Paradise Valley, Arizona\n",
      "Pulling data for:  Parker, Arizona\n",
      "Pulling data for:  Patagonia, Arizona\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling data for:  Payson, Arizona\n",
      "Pulling data for:  Peoria, Arizona\n",
      "Pulling data for:  Phoenix, Arizona\n",
      "Pulling data for:  Pima, Arizona\n",
      "Pulling data for:  Pinetop-Lakeside, Arizona\n",
      "Pulling data for:  Prescott, Arizona\n",
      "Pulling data for:  Prescott Valley, Arizona\n",
      "Pulling data for:  Quartzsite, Arizona\n",
      "Pulling data for:  Queen Creek, Arizona\n",
      "Pulling data for:  Safford, Arizona\n",
      "Pulling data for:  Sahuarita, Arizona\n",
      "Pulling data for:  San Luis, Arizona\n",
      "Pulling data for:  Scottsdale, Arizona\n",
      "Pulling data for:  Sedona, Arizona\n",
      "Pulling data for:  Show Low, Arizona\n",
      "Pulling data for:  Sierra Vista, Arizona\n",
      "Pulling data for:  Snowflake, Arizona\n",
      "Pulling data for:  Somerton, Arizona\n",
      "Pulling data for:  South Tucson, Arizona\n",
      "Pulling data for:  Springerville, Arizona\n",
      "Pulling data for:  St. Johns, Arizona\n",
      "Pulling data for:  Star Valley, Arizona\n",
      "Pulling data for:  Superior, Arizona\n",
      "Pulling data for:  Surprise, Arizona\n",
      "Pulling data for:  Taylor, Arizona\n",
      "Pulling data for:  Tempe, Arizona\n",
      "Pulling data for:  Thatcher, Arizona\n",
      "Pulling data for:  Tolleson, Arizona\n",
      "Pulling data for:  Tombstone, Arizona\n",
      "Pulling data for:  Tucson, Arizona\n",
      "Pulling data for:  Tusayan, Arizona\n",
      "Pulling data for:  Wellton, Arizona\n",
      "Pulling data for:  Wickenburg, Arizona\n",
      "Pulling data for:  Willcox, Arizona\n",
      "Pulling data for:  Williams, Arizona\n",
      "Pulling data for:  Winkelman, Arizona\n",
      "Pulling data for:  Winslow, Arizona\n",
      "Pulling data for:  Youngtown, Arizona\n",
      "Pulling data for:  Yuma, Arizona\n",
      "Pulling data for:  Adona, Arkansas\n",
      "Pulling data for:  Alexander, Arkansas\n",
      "Pulling data for:  Alicia, Arkansas\n",
      "Pulling data for:  Allport, Arkansas\n",
      "Pulling data for:  Alma, Arkansas\n",
      "Pulling data for:  Almyra, Arkansas\n",
      "Pulling data for:  Alpena, Arkansas\n",
      "Pulling data for:  Altheimer, Arkansas\n",
      "Pulling data for:  Altus, Arkansas\n",
      "Pulling data for:  Amagon, Arkansas\n",
      "Pulling data for:  Amity, Arkansas\n",
      "Pulling data for:  Anthonyville, Arkansas\n",
      "Pulling data for:  Antoine, Arkansas\n",
      "Pulling data for:  Arkadelphia, Arkansas\n",
      "Pulling data for:  Arkansas City, Arkansas\n",
      "Pulling data for:  Ash Flat, Arkansas\n",
      "Pulling data for:  Ashdown, Arkansas\n",
      "Pulling data for:  Atkins, Arkansas\n",
      "Pulling data for:  Aubrey, Arkansas\n",
      "Pulling data for:  Augusta, Arkansas\n",
      "Pulling data for:  Austin, Arkansas\n",
      "Pulling data for:  Avoca, Arkansas\n",
      "Pulling data for:  Bald Knob, Arkansas\n",
      "Pulling data for:  Banks, Arkansas\n",
      "Pulling data for:  Barling, Arkansas\n",
      "Pulling data for:  Bassett, Arkansas\n",
      "Pulling data for:  Batesville, Arkansas\n",
      "Pulling data for:  Bauxite, Arkansas\n",
      "Pulling data for:  Bay, Arkansas\n",
      "Pulling data for:  Bearden, Arkansas\n",
      "Pulling data for:  Beaver, Arkansas\n",
      "Pulling data for:  Beebe, Arkansas\n",
      "Pulling data for:  Beedeville, Arkansas\n",
      "Pulling data for:  Bella Vista, Arkansas\n",
      "Pulling data for:  Bellefonte, Arkansas\n",
      "Pulling data for:  Belleville, Arkansas\n",
      "Pulling data for:  Ben Lomond, Arkansas\n",
      "Pulling data for:  Benton, Arkansas\n",
      "Pulling data for:  Bentonville, Arkansas\n",
      "Pulling data for:  Bergman, Arkansas\n",
      "Pulling data for:  Berryville, Arkansas\n",
      "Pulling data for:  Bethel Heights, Arkansas\n",
      "Pulling data for:  Big Flat, Arkansas\n",
      "Pulling data for:  Bigelow, Arkansas\n",
      "Pulling data for:  Biggers, Arkansas\n",
      "Pulling data for:  Birdsong, Arkansas\n",
      "Pulling data for:  Black Oak, Arkansas\n",
      "Pulling data for:  Black Rock, Arkansas\n",
      "Pulling data for:  Black Springs, Arkansas\n",
      "Pulling data for:  Blevins, Arkansas\n",
      "Pulling data for:  Blue Eye, Arkansas\n",
      "Pulling data for:  Blue Mountain, Arkansas\n",
      "Pulling data for:  Bluff City, Arkansas\n",
      "Pulling data for:  Blytheville, Arkansas\n",
      "Pulling data for:  Bodcaw, Arkansas\n",
      "Pulling data for:  Bonanza, Arkansas\n",
      "Pulling data for:  Bono, Arkansas\n",
      "Pulling data for:  Booneville, Arkansas\n",
      "Pulling data for:  Bradford, Arkansas\n",
      "Pulling data for:  Bradley, Arkansas\n",
      "Pulling data for:  Branch, Arkansas\n",
      "Pulling data for:  Briarcliff, Arkansas\n",
      "Pulling data for:  Brinkley, Arkansas\n",
      "Pulling data for:  Brookland, Arkansas\n",
      "Pulling data for:  Bryant, Arkansas\n",
      "Pulling data for:  Buckner, Arkansas\n",
      "Pulling data for:  Bull Shoals, Arkansas\n",
      "Pulling data for:  Burdette, Arkansas\n",
      "Pulling data for:  Cabot, Arkansas\n",
      "Pulling data for:  Caddo Valley, Arkansas\n",
      "Pulling data for:  Caldwell, Arkansas\n",
      "Pulling data for:  Cale, Arkansas\n",
      "Pulling data for:  Calico Rock, Arkansas\n",
      "Pulling data for:  Calion, Arkansas\n",
      "Pulling data for:  Camden, Arkansas\n",
      "Pulling data for:  Cammack Village, Arkansas\n",
      "Pulling data for:  Campbell Station, Arkansas\n",
      "Pulling data for:  Caraway, Arkansas\n",
      "Pulling data for:  Carlisle, Arkansas\n",
      "Pulling data for:  Carthage, Arkansas\n",
      "Pulling data for:  Casa, Arkansas\n",
      "Pulling data for:  Cash, Arkansas\n",
      "Pulling data for:  Caulksville, Arkansas\n",
      "Pulling data for:  Cave City, Arkansas\n",
      "Pulling data for:  Cave Springs, Arkansas\n",
      "Pulling data for:  Cedarville, Arkansas\n",
      "Pulling data for:  Centerton, Arkansas\n",
      "Pulling data for:  Central City, Arkansas\n",
      "Pulling data for:  Charleston, Arkansas\n",
      "Pulling data for:  Cherokee Village, Arkansas\n",
      "Pulling data for:  Cherry Valley, Arkansas\n",
      "Pulling data for:  Chester, Arkansas\n",
      "Pulling data for:  Chidester, Arkansas\n",
      "Pulling data for:  Clarendon, Arkansas\n",
      "Pulling data for:  Clarkedale, Arkansas\n",
      "Pulling data for:  Clarksville, Arkansas\n",
      "Pulling data for:  Clinton, Arkansas\n",
      "Pulling data for:  Coal Hill, Arkansas\n",
      "Pulling data for:  Colt, Arkansas\n",
      "Pulling data for:  Concord, Arkansas\n",
      "Pulling data for:  Conway, Arkansas\n",
      "Pulling data for:  Corinth, Arkansas\n",
      "Pulling data for:  Corning, Arkansas\n",
      "Pulling data for:  Cotter, Arkansas\n",
      "Pulling data for:  Cotton Plant, Arkansas\n",
      "Pulling data for:  Cove, Arkansas\n",
      "Pulling data for:  Coy, Arkansas\n",
      "Pulling data for:  Crawfordsville, Arkansas\n",
      "Pulling data for:  Crossett, Arkansas\n",
      "Pulling data for:  Cushman, Arkansas\n",
      "Pulling data for:  Daisy, Arkansas\n",
      "Pulling data for:  Damascus, Arkansas\n",
      "Pulling data for:  Danville, Arkansas\n",
      "Pulling data for:  Dardanelle, Arkansas\n",
      "Pulling data for:  Datto, Arkansas\n",
      "Pulling data for:  De Queen, Arkansas\n",
      "Pulling data for:  DeValls Bluff, Arkansas\n",
      "Pulling data for:  Decatur, Arkansas\n",
      "Pulling data for:  Delaplaine, Arkansas\n",
      "Pulling data for:  Delight, Arkansas\n",
      "Pulling data for:  Dell, Arkansas\n",
      "Pulling data for:  Denning, Arkansas\n",
      "Pulling data for:  Dermott, Arkansas\n",
      "Pulling data for:  Des Arc, Arkansas\n",
      "Pulling data for:  DeWitt, Arkansas\n",
      "Pulling data for:  Diamond City, Arkansas\n",
      "Pulling data for:  Diaz, Arkansas\n",
      "Pulling data for:  Dierks, Arkansas\n",
      "Pulling data for:  Donaldson, Arkansas\n",
      "Pulling data for:  Dover, Arkansas\n",
      "Pulling data for:  Dumas, Arkansas\n",
      "Pulling data for:  Dyer, Arkansas\n",
      "Pulling data for:  Dyess, Arkansas\n",
      "Pulling data for:  Earle, Arkansas\n",
      "Pulling data for:  East Camden, Arkansas\n",
      "Pulling data for:  Edmondson, Arkansas\n",
      "Pulling data for:  Egypt, Arkansas\n",
      "Pulling data for:  El Dorado, Arkansas\n",
      "Pulling data for:  Elaine, Arkansas\n",
      "Pulling data for:  Elkins, Arkansas\n",
      "Pulling data for:  Elm Springs, Arkansas\n",
      "Pulling data for:  Emerson, Arkansas\n",
      "Pulling data for:  Emmet, Arkansas\n",
      "Pulling data for:  England, Arkansas\n",
      "Pulling data for:  Enola, Arkansas\n",
      "Pulling data for:  Etowah, Arkansas\n",
      "Pulling data for:  Eudora, Arkansas\n",
      "Pulling data for:  Eureka Springs, Arkansas\n",
      "Pulling data for:  Evening Shade, Arkansas\n",
      "Pulling data for:  Everton, Arkansas\n",
      "Pulling data for:  Fairfield Bay, Arkansas\n",
      "Pulling data for:  Fargo, Arkansas\n",
      "Pulling data for:  Farmington, Arkansas\n",
      "Pulling data for:  Fayetteville, Arkansas\n",
      "Pulling data for:  Felsenthal, Arkansas\n",
      "Pulling data for:  Fifty-Six, Arkansas\n",
      "Pulling data for:  Fisher, Arkansas\n",
      "Pulling data for:  Flippin, Arkansas\n",
      "Pulling data for:  Fordyce, Arkansas\n",
      "Pulling data for:  Foreman, Arkansas\n",
      "Pulling data for:  Forrest City, Arkansas\n",
      "Pulling data for:  Fort Smith, Arkansas\n",
      "Pulling data for:  Fouke, Arkansas\n",
      "Pulling data for:  Fountain Hill, Arkansas\n",
      "Pulling data for:  Fountain Lake, Arkansas\n",
      "Pulling data for:  Fourche, Arkansas\n",
      "Pulling data for:  Franklin, Arkansas\n",
      "Pulling data for:  Fredonia (Biscoe), Arkansas\n",
      "Pulling data for:  Friendship, Arkansas\n",
      "Pulling data for:  Fulton, Arkansas\n",
      "Pulling data for:  Garfield, Arkansas\n",
      "Pulling data for:  Garland, Arkansas\n",
      "Pulling data for:  Garner, Arkansas\n",
      "Pulling data for:  Gassville, Arkansas\n",
      "Pulling data for:  Gateway, Arkansas\n",
      "Pulling data for:  Gentry, Arkansas\n",
      "Pulling data for:  Georgetown, Arkansas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling data for:  Gilbert, Arkansas\n",
      "Pulling data for:  Gillett, Arkansas\n",
      "Pulling data for:  Gillham, Arkansas\n",
      "Pulling data for:  Gilmore, Arkansas\n",
      "Pulling data for:  Glenwood, Arkansas\n",
      "Pulling data for:  Goshen, Arkansas\n",
      "Pulling data for:  Gosnell, Arkansas\n",
      "Pulling data for:  Gould, Arkansas\n",
      "Pulling data for:  Grady, Arkansas\n",
      "Pulling data for:  Grannis, Arkansas\n",
      "Pulling data for:  Gravette, Arkansas\n",
      "Pulling data for:  Green Forest, Arkansas\n",
      "Pulling data for:  Greenbrier, Arkansas\n",
      "Pulling data for:  Greenland, Arkansas\n",
      "Pulling data for:  Greenway, Arkansas\n",
      "Pulling data for:  Greenwood, Arkansas\n",
      "Pulling data for:  Greers Ferry, Arkansas\n",
      "Pulling data for:  Griffithville, Arkansas\n",
      "Pulling data for:  Grubbs, Arkansas\n",
      "Pulling data for:  Guion, Arkansas\n",
      "Pulling data for:  Gum Springs, Arkansas\n",
      "Pulling data for:  Gurdon, Arkansas\n",
      "Pulling data for:  Guy, Arkansas\n",
      "Pulling data for:  Hackett, Arkansas\n",
      "Pulling data for:  Hamburg, Arkansas\n",
      "Pulling data for:  Hampton, Arkansas\n",
      "Pulling data for:  Hardy, Arkansas\n",
      "Pulling data for:  Harrell, Arkansas\n",
      "Pulling data for:  Harrisburg, Arkansas\n",
      "Pulling data for:  Harrison, Arkansas\n",
      "Pulling data for:  Hartford, Arkansas\n",
      "Pulling data for:  Hartman, Arkansas\n",
      "Pulling data for:  Haskell, Arkansas\n",
      "Pulling data for:  Hatfield, Arkansas\n",
      "Pulling data for:  Havana, Arkansas\n",
      "Pulling data for:  Haynes, Arkansas\n",
      "Pulling data for:  Hazen, Arkansas\n",
      "Pulling data for:  Heber Springs, Arkansas\n",
      "Pulling data for:  Hector, Arkansas\n",
      "Pulling data for:  Helena-West Helena, Arkansas\n",
      "Pulling data for:  Hermitage, Arkansas\n",
      "Pulling data for:  Hickory Ridge, Arkansas\n",
      "Pulling data for:  Higden, Arkansas\n",
      "Pulling data for:  Higginson, Arkansas\n",
      "Pulling data for:  Highfill, Arkansas\n",
      "Pulling data for:  Highland, Arkansas\n",
      "Pulling data for:  Hindsville, Arkansas\n",
      "Pulling data for:  Holland, Arkansas\n",
      "Pulling data for:  Holly Grove, Arkansas\n",
      "Pulling data for:  Hope, Arkansas\n",
      "Pulling data for:  Horatio, Arkansas\n",
      "Pulling data for:  Horseshoe Bend, Arkansas\n",
      "Pulling data for:  Horseshoe Lake, Arkansas\n",
      "Pulling data for:  Hot Springs, Arkansas\n",
      "Pulling data for:  Houston, Arkansas\n",
      "Pulling data for:  Hoxie, Arkansas\n",
      "Pulling data for:  Hughes, Arkansas\n",
      "Pulling data for:  Humnoke, Arkansas\n",
      "Pulling data for:  Humphrey, Arkansas\n",
      "Pulling data for:  Hunter, Arkansas\n",
      "Pulling data for:  Huntington, Arkansas\n",
      "Pulling data for:  Huntsville, Arkansas\n",
      "Pulling data for:  Huttig, Arkansas\n",
      "Pulling data for:  Imboden, Arkansas\n",
      "Pulling data for:  Jacksonport, Arkansas\n",
      "Pulling data for:  Jacksonville, Arkansas\n",
      "Pulling data for:  Jasper, Arkansas\n",
      "Pulling data for:  Jennette, Arkansas\n",
      "Pulling data for:  Jericho, Arkansas\n",
      "Pulling data for:  Johnson, Arkansas\n",
      "Pulling data for:  Joiner, Arkansas\n",
      "Pulling data for:  Jonesboro, Arkansas\n",
      "Pulling data for:  Judsonia, Arkansas\n",
      "Pulling data for:  Junction City, Arkansas\n",
      "Pulling data for:  Keiser, Arkansas\n",
      "Pulling data for:  Kensett, Arkansas\n",
      "Pulling data for:  Keo, Arkansas\n",
      "Pulling data for:  Kibler, Arkansas\n",
      "Pulling data for:  Kingsland, Arkansas\n",
      "Pulling data for:  Knobel, Arkansas\n",
      "Pulling data for:  Knoxville, Arkansas\n",
      "Pulling data for:  Lafe, Arkansas\n",
      "Pulling data for:  LaGrange, Arkansas\n",
      "Pulling data for:  Lake City, Arkansas\n",
      "Pulling data for:  Lake View, Arkansas\n",
      "Pulling data for:  Lake Village, Arkansas\n",
      "Pulling data for:  Lakeview, Arkansas\n",
      "Pulling data for:  Lamar, Arkansas\n",
      "Pulling data for:  Lavaca, Arkansas\n",
      "Pulling data for:  Leachville, Arkansas\n",
      "Pulling data for:  Lead Hill, Arkansas\n",
      "Pulling data for:  Leola, Arkansas\n",
      "Pulling data for:  Lepanto, Arkansas\n",
      "Pulling data for:  Leslie, Arkansas\n",
      "Pulling data for:  Letona, Arkansas\n",
      "Pulling data for:  Lewisville, Arkansas\n",
      "Pulling data for:  Lexa, Arkansas\n",
      "Pulling data for:  Lincoln, Arkansas\n",
      "Pulling data for:  Little Flock, Arkansas\n",
      "Pulling data for:  Little Rock, Arkansas\n",
      "Pulling data for:  Lockesburg, Arkansas\n",
      "Pulling data for:  London, Arkansas\n",
      "Pulling data for:  Lonoke, Arkansas\n",
      "Pulling data for:  Lonsdale, Arkansas\n",
      "Pulling data for:  Louann, Arkansas\n",
      "Pulling data for:  Lowell, Arkansas\n",
      "Pulling data for:  Luxora, Arkansas\n",
      "Pulling data for:  Lynn, Arkansas\n",
      "Pulling data for:  Madison, Arkansas\n",
      "Pulling data for:  Magazine, Arkansas\n",
      "Pulling data for:  Magness, Arkansas\n",
      "Pulling data for:  Magnolia, Arkansas\n",
      "Pulling data for:  Malvern, Arkansas\n",
      "Pulling data for:  Mammoth Spring, Arkansas\n",
      "Pulling data for:  Manila, Arkansas\n",
      "Pulling data for:  Mansfield, Arkansas\n",
      "Pulling data for:  Marianna, Arkansas\n",
      "Pulling data for:  Marie, Arkansas\n",
      "Pulling data for:  Marion, Arkansas\n",
      "Pulling data for:  Marked Tree, Arkansas\n",
      "Pulling data for:  Marmaduke, Arkansas\n",
      "Pulling data for:  Marshall, Arkansas\n",
      "Pulling data for:  Marvell, Arkansas\n",
      "Pulling data for:  Maumelle, Arkansas\n",
      "Pulling data for:  Mayflower, Arkansas\n",
      "Pulling data for:  Maynard, Arkansas\n",
      "Pulling data for:  McCaskill, Arkansas\n",
      "Pulling data for:  McCrory, Arkansas\n",
      "Pulling data for:  McDougal, Arkansas\n",
      "Pulling data for:  McGehee, Arkansas\n",
      "Pulling data for:  McNab, Arkansas\n",
      "Pulling data for:  McNeil, Arkansas\n",
      "Pulling data for:  McRae, Arkansas\n",
      "Pulling data for:  Melbourne, Arkansas\n",
      "Pulling data for:  Mena, Arkansas\n",
      "Pulling data for:  Menifee, Arkansas\n",
      "Pulling data for:  Midland, Arkansas\n",
      "Pulling data for:  Midway, Hot Spring County, Arkansas\n",
      "Pulling data for:  Mineral Springs, Arkansas\n",
      "Pulling data for:  Minturn, Arkansas\n",
      "Pulling data for:  Mitchellville, Arkansas\n",
      "Pulling data for:  Monette, Arkansas\n",
      "Pulling data for:  Monticello, Arkansas\n",
      "Pulling data for:  Montrose, Arkansas\n",
      "Pulling data for:  Moorefield, Arkansas\n",
      "Pulling data for:  Moro, Arkansas\n",
      "Pulling data for:  Morrilton, Arkansas\n",
      "Pulling data for:  Morrison Bluff, Arkansas\n",
      "Pulling data for:  Mount Ida, Arkansas\n",
      "Pulling data for:  Mount Pleasant, Arkansas\n",
      "Pulling data for:  Mount Vernon, Arkansas\n",
      "Pulling data for:  Mountain Home, Arkansas\n",
      "Pulling data for:  Mountain Pine, Arkansas\n",
      "Pulling data for:  Mountain View, Arkansas\n",
      "Pulling data for:  Mountainburg, Arkansas\n",
      "Pulling data for:  Mulberry, Arkansas\n",
      "Pulling data for:  Murfreesboro, Arkansas\n",
      "Pulling data for:  Nashville, Arkansas\n",
      "Pulling data for:  Newark, Arkansas\n",
      "Pulling data for:  Newport, Arkansas\n",
      "Pulling data for:  Nimmons, Arkansas\n",
      "Pulling data for:  Norfork, Arkansas\n",
      "Pulling data for:  Norman, Arkansas\n",
      "Pulling data for:  Norphlet, Arkansas\n",
      "Pulling data for:  North Little Rock, Arkansas\n",
      "Pulling data for:  O'Kean, Arkansas\n",
      "Pulling data for:  Oak Grove, Carroll County, Arkansas\n",
      "Pulling data for:  Oak Grove Heights, Arkansas\n",
      "Pulling data for:  Oakhaven, Arkansas\n",
      "Pulling data for:  Oden, Arkansas\n",
      "Pulling data for:  Ogden, Arkansas\n",
      "Pulling data for:  Oil Trough, Arkansas\n",
      "Pulling data for:  Okolona, Arkansas\n",
      "Pulling data for:  Ola, Arkansas\n",
      "Pulling data for:  Omaha, Arkansas\n",
      "Pulling data for:  Oppelo, Arkansas\n",
      "Pulling data for:  Osceola, Arkansas\n",
      "Pulling data for:  Oxford, Arkansas\n",
      "Pulling data for:  Ozan, Arkansas\n",
      "Pulling data for:  Ozark, Arkansas\n",
      "Pulling data for:  Palestine, Arkansas\n",
      "Pulling data for:  Pangburn, Arkansas\n",
      "Pulling data for:  Paragould, Arkansas\n",
      "Pulling data for:  Paris, Arkansas\n",
      "Pulling data for:  Parkdale, Arkansas\n",
      "Pulling data for:  Parkin, Arkansas\n",
      "Pulling data for:  Patmos, Arkansas\n",
      "Pulling data for:  Patterson, Arkansas\n",
      "Pulling data for:  Pea Ridge, Arkansas\n",
      "Pulling data for:  Peach Orchard, Arkansas\n",
      "Pulling data for:  Perla, Arkansas\n",
      "Pulling data for:  Perry, Arkansas\n",
      "Pulling data for:  Perrytown, Arkansas\n",
      "Pulling data for:  Perryville, Arkansas\n",
      "Pulling data for:  Piggott, Arkansas\n",
      "Pulling data for:  Pindall, Arkansas\n",
      "Pulling data for:  Pine Bluff, Arkansas\n",
      "Pulling data for:  Pineville, Arkansas\n",
      "Pulling data for:  Plainview, Arkansas\n",
      "Pulling data for:  Pleasant Plains, Arkansas\n",
      "Pulling data for:  Plumerville, Arkansas\n",
      "Pulling data for:  Pocahontas, Arkansas\n",
      "Pulling data for:  Pollard, Arkansas\n",
      "Pulling data for:  Portia, Arkansas\n",
      "Pulling data for:  Portland, Arkansas\n",
      "Pulling data for:  Pottsville, Arkansas\n",
      "Pulling data for:  Powhatan, Arkansas\n",
      "Pulling data for:  Poyen, Arkansas\n",
      "Pulling data for:  Prairie Grove, Arkansas\n",
      "Pulling data for:  Prattsville, Arkansas\n",
      "Pulling data for:  Prescott, Arkansas\n",
      "Pulling data for:  Pyatt, Arkansas\n",
      "Pulling data for:  Quitman, Arkansas\n",
      "Pulling data for:  Ratcliff, Arkansas\n",
      "Pulling data for:  Ravenden, Arkansas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling data for:  Ravenden Springs, Arkansas\n",
      "Pulling data for:  Rector, Arkansas\n",
      "Pulling data for:  Redfield, Arkansas\n",
      "Pulling data for:  Reed, Arkansas\n",
      "Pulling data for:  Reyno, Arkansas\n",
      "Pulling data for:  Rison, Arkansas\n",
      "Pulling data for:  Rockport, Arkansas\n",
      "Pulling data for:  Roe, Arkansas\n",
      "Pulling data for:  Rogers, Arkansas\n",
      "Pulling data for:  Rondo, Arkansas\n",
      "Pulling data for:  Rose Bud, Arkansas\n",
      "Pulling data for:  Rosston, Arkansas\n",
      "Pulling data for:  Rudy, Arkansas\n",
      "Pulling data for:  Russell, Arkansas\n",
      "Pulling data for:  Russellville, Arkansas\n",
      "Pulling data for:  Salem, Fulton County, Arkansas\n",
      "Pulling data for:  Salesville, Arkansas\n",
      "Pulling data for:  Scranton, Arkansas\n",
      "Pulling data for:  Searcy, Arkansas\n",
      "Pulling data for:  Sedgwick, Arkansas\n",
      "Pulling data for:  Shannon Hills, Arkansas\n",
      "Pulling data for:  Sheridan, Arkansas\n",
      "Pulling data for:  Sherrill, Arkansas\n",
      "Pulling data for:  Sherwood, Arkansas\n",
      "Pulling data for:  Shirley, Arkansas\n",
      "Pulling data for:  Sidney, Arkansas\n",
      "Pulling data for:  Siloam Springs, Arkansas\n",
      "Pulling data for:  Smackover, Arkansas\n",
      "Pulling data for:  Smithville, Arkansas\n",
      "Pulling data for:  South Lead Hill, Arkansas\n",
      "Pulling data for:  Southside, Independence County, Arkansas\n",
      "Pulling data for:  Sparkman, Arkansas\n",
      "Pulling data for:  Springdale, Arkansas\n",
      "Pulling data for:  Springtown, Arkansas\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-1c7593918869>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Pulling data for: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0marticle_title\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Make a request for each article title\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequest_pageinfo_per_article\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_title\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-6b79d3a22223>\u001b[0m in \u001b[0;36mrequest_pageinfo_per_article\u001b[1;34m(article_title, endpoint_url, request_template, headers)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mAPI_THROTTLE_WAIT\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAPI_THROTTLE_WAIT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendpoint_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest_template\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mjson_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    540\u001b[0m         }\n\u001b[0;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    443\u001b[0m                     \u001b[1;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1345\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1347\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1348\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1239\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1240\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1241\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1242\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the responses for each article\n",
    "article_responses = []\n",
    "\n",
    "# To store failed articles\n",
    "failed_articles = []  \n",
    "\n",
    "# Iterate through values from us_citiesxstates.page_title\n",
    "\n",
    "for article_title in us_citiesxstates.page_title:\n",
    "    print('Pulling data for: ',article_title)\n",
    "    # Make a request for each article title\n",
    "    response = request_pageinfo_per_article(article_title)\n",
    "    \n",
    "    if response is not None:\n",
    "        # Append the response to the list as a dictionary\n",
    "        article_responses.append(response)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {article_title}\")\n",
    "        failed_articles.append(article_title)\n",
    "\n",
    "\n",
    "# Convert the responses into a dataframe.\n",
    "        \n",
    "article_responses_df = pd.DataFrame(article_responses)\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "for index, row in article_responses_df.iterrows():\n",
    "    page_info = row['query']\n",
    "    page_id = list(page_info['pages'].keys())[0]\n",
    "    page_data = page_info['pages'][page_id]\n",
    "\n",
    "    # Convert the page_data dictionary into a DataFrame with one row\n",
    "    page_data_df = pd.DataFrame.from_dict(page_data, orient='index').T\n",
    "    result_df = pd.concat([result_df, page_data_df])\n",
    "\n",
    "# Reset the index\n",
    "result_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of failed articles for which the API failed\n",
    "failed_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export page info request as csv (intermediate file) \n",
    "result_df.to_csv(os.path.join(cwd, 'intermediate/article_info.csv'), index=False)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Getting Article Quality Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LiftWing ORES API Configuration:\n",
    "\n",
    "This section defines the configuration for making API requests to the LiftWing ORES endpoint and prediction model.\n",
    "\n",
    "- `API_ORES_LIFTWING_ENDPOINT`: The API endpoint for the LiftWing ORES model.\n",
    "\n",
    "- `API_ORES_EN_QUALITY_MODEL`: The specific quality model for English Wikipedia articles.\n",
    "\n",
    "#### Throttling and Rate Limits:\n",
    "\n",
    "The code outlines the calculation of throttling and rate limits for making automated requests to the API.\n",
    "\n",
    "- `API_LATENCY_ASSUMED`: The assumed API latency in seconds.\n",
    "\n",
    "- `API_THROTTLE_WAIT`: The calculated time interval to wait between API requests to comply with rate limits.\n",
    "\n",
    "#### Request Headers:\n",
    "\n",
    "This part provides details about the headers to be included in API requests, including:\n",
    "\n",
    "- `User-Agent`: A user-agent string indicating the requester's identity.\n",
    "\n",
    "- `Content-Type`: The type of content in the request.\n",
    "\n",
    "- `Authorization`: The authorization header with a bearer token.\n",
    "\n",
    "#### Header Parameters Template:\n",
    "\n",
    "The template for header parameters that need to be supplied in API requests, including email address and access token.\n",
    "\n",
    "#### Sample Article Revisions:\n",
    "\n",
    "A dictionary of English Wikipedia article titles and sample revision IDs for use in ORES scoring examples.\n",
    "\n",
    "#### ORES Request Data Template:\n",
    "\n",
    "The template for data payload required when making a scoring request to the ORES model.\n",
    "\n",
    "#### User Information:\n",
    "\n",
    "Your username and access token for making authenticated API requests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    The current LiftWing ORES API endpoint and prediction model\n",
    "API_ORES_LIFTWING_ENDPOINT = \"https://api.wikimedia.org/service/lw/inference/v1/models/{model_name}:predict\"\n",
    "API_ORES_EN_QUALITY_MODEL = \"enwiki-articlequality\"\n",
    "\n",
    "#\n",
    "#    The throttling rate is a function of the Access token that you are granted when you request the token. The constants\n",
    "#    come from dissecting the token and getting the rate limits from the granted token. An example of that is below.\n",
    "#\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (60.0/5000.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "#    When making automated requests we should include something that is unique to the person making the request\n",
    "#    This should include an email - your UW email would be good to put in there\n",
    "#    \n",
    "#    Because all LiftWing API requests require some form of authentication, you need to provide your access token\n",
    "#    as part of the header too\n",
    "#\n",
    "REQUEST_HEADER_TEMPLATE = {\n",
    "    'User-Agent': \"<adi279@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2023\",\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': \"Bearer {access_token}\"\n",
    "}\n",
    "#\n",
    "#    This is a template for the parameters that we need to supply in the headers of an API request\n",
    "#\n",
    "REQUEST_HEADER_PARAMS_TEMPLATE = {\n",
    "    'email_address' : \"adithyaa279@gmail.com\",         # your email address should go here\n",
    "    'access_token'  : \"eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJhdWQiOiI0NmQ4ZTg5NzY5M2YxYTE3OGU4ZDViZTQ4ZWY5Njg4OCIsImp0aSI6IjM4YTUwMDZjNTYxOGYxZWEyMDM0OWM4ZTkzNjQyNmUwZjhjNzBiMThjNzBmMzNlZDA2N2FiNzM0ZDFmYTVhMGY4MDc5YjAwMmNkYmI4N2QzIiwiaWF0IjoxNjk3MzM1NjE2LjAwNDY5LCJuYmYiOjE2OTczMzU2MTYuMDA0NjkzLCJleHAiOjMzMjU0MjQ0NDE2LjAwMzM1LCJzdWIiOiI3NDAwNjUzOCIsImlzcyI6Imh0dHBzOi8vbWV0YS53aWtpbWVkaWEub3JnIiwicmF0ZWxpbWl0Ijp7InJlcXVlc3RzX3Blcl91bml0Ijo1MDAwLCJ1bml0IjoiSE9VUiJ9LCJzY29wZXMiOlsiYmFzaWMiLCJjcmVhdGVlZGl0bW92ZXBhZ2UiLCJlZGl0cHJvdGVjdGVkIl19.U1V2FY9Zh11AaHTjy1OG6ql1gkKcsUhtwid9j9-1_L7Fwm2BaHl_1aOBNMhxC2TZMjAIs9hagRkKUug3TI9EMP-Up89cPwsiCdnb7rZFSXxU5hUIrrpVVN49y8LonBZeAy2VXCAnbeHCGQkfYbVBf92JOhY7rpUEGiMGJddXnLDjHmKBX2fOT31Kf-9pb4UUGt1QcJxucnM094KVJfjL5mdvWSscqdKRwc982Qa951uifNkAFse4uxVeq10gwUIgO_sfGVvU3FnAvGl7r3BmnmUsrVCVb_NwnWpDye8JYQ6aNOLxsSCS9iw13PVct1369ZdO__wiNCFX9BOiG9C7juosMu9X2XoBs35eXUOIhZOcbHRhPatfP1kvzpEqAuvBiaXTshVI7TA_FWgg2htpIieVFuKtD0X6koQ-5LcInL_re2PuFrovMJOtrV_k2m8Qx4JYntlaxgZJc-mNW-wbgT4LUIC03MR9s8ITvbiYY2q96nwqPsJQIH0JTa_49eDAh4L8sjXD-ktXtnyTpQtulJzQn-QduWFO8o_1ssjhbAgIecp7HzTIxDOdlIBG5Ib24IA4VRk8zC20OKWVjK5EQB0jbyLH4rv2IizXkzUrhadpZPRk8VHECkM3sbMi97U-m_5uQ3Mv1-Cpv0wtdl31kNR0h3C-hS6GZ0zr_apsn50\"          # the access token you create will need to go here\n",
    "}\n",
    "\n",
    "#\n",
    "#    A dictionary of English Wikipedia article titles (keys) and sample revision IDs that can be used for this ORES scoring example\n",
    "#\n",
    "# ARTICLE_REVISIONS = { 'Bison':1085687913 , 'Northern flicker':1086582504 , 'Red squirrel':1083787665 , 'Chinook salmon':1085406228 , 'Horseshoe bat':1060601936 }\n",
    "\n",
    "#\n",
    "#    This is a template of the data required as a payload when making a scoring request of the ORES model\n",
    "#\n",
    "ORES_REQUEST_DATA_TEMPLATE = {\n",
    "    \"lang\":        \"en\",     # required that its english - we're scoring English Wikipedia revisions\n",
    "    \"rev_id\":      \"\",       # this request requires a revision id\n",
    "    \"features\":    True\n",
    "}\n",
    "\n",
    "#\n",
    "#    These are used later - defined here so they, at least, have empty values\n",
    "#\n",
    "USERNAME = \"Adithyaavaasen\"\n",
    "ACCESS_TOKEN = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJhdWQiOiI0NmQ4ZTg5NzY5M2YxYTE3OGU4ZDViZTQ4ZWY5Njg4OCIsImp0aSI6IjM4YTUwMDZjNTYxOGYxZWEyMDM0OWM4ZTkzNjQyNmUwZjhjNzBiMThjNzBmMzNlZDA2N2FiNzM0ZDFmYTVhMGY4MDc5YjAwMmNkYmI4N2QzIiwiaWF0IjoxNjk3MzM1NjE2LjAwNDY5LCJuYmYiOjE2OTczMzU2MTYuMDA0NjkzLCJleHAiOjMzMjU0MjQ0NDE2LjAwMzM1LCJzdWIiOiI3NDAwNjUzOCIsImlzcyI6Imh0dHBzOi8vbWV0YS53aWtpbWVkaWEub3JnIiwicmF0ZWxpbWl0Ijp7InJlcXVlc3RzX3Blcl91bml0Ijo1MDAwLCJ1bml0IjoiSE9VUiJ9LCJzY29wZXMiOlsiYmFzaWMiLCJjcmVhdGVlZGl0bW92ZXBhZ2UiLCJlZGl0cHJvdGVjdGVkIl19.U1V2FY9Zh11AaHTjy1OG6ql1gkKcsUhtwid9j9-1_L7Fwm2BaHl_1aOBNMhxC2TZMjAIs9hagRkKUug3TI9EMP-Up89cPwsiCdnb7rZFSXxU5hUIrrpVVN49y8LonBZeAy2VXCAnbeHCGQkfYbVBf92JOhY7rpUEGiMGJddXnLDjHmKBX2fOT31Kf-9pb4UUGt1QcJxucnM094KVJfjL5mdvWSscqdKRwc982Qa951uifNkAFse4uxVeq10gwUIgO_sfGVvU3FnAvGl7r3BmnmUsrVCVb_NwnWpDye8JYQ6aNOLxsSCS9iw13PVct1369ZdO__wiNCFX9BOiG9C7juosMu9X2XoBs35eXUOIhZOcbHRhPatfP1kvzpEqAuvBiaXTshVI7TA_FWgg2htpIieVFuKtD0X6koQ-5LcInL_re2PuFrovMJOtrV_k2m8Qx4JYntlaxgZJc-mNW-wbgT4LUIC03MR9s8ITvbiYY2q96nwqPsJQIH0JTa_49eDAh4L8sjXD-ktXtnyTpQtulJzQn-QduWFO8o_1ssjhbAgIecp7HzTIxDOdlIBG5Ib24IA4VRk8zC20OKWVjK5EQB0jbyLH4rv2IizXkzUrhadpZPRk8VHECkM3sbMi97U-m_5uQ3Mv1-Cpv0wtdl31kNR0h3C-hS6GZ0zr_apsn50\"\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "##   Decode the Wikimedia JWT Access token\n",
    "##\n",
    "##   NOTE: This is not required to use LiftWing to request ORES scores. This is just being done to satisfy my curiosity.\n",
    "##\n",
    "#import base64\n",
    "#\n",
    "#print(\"Decoding the ACCESS_TOKEN:\")\n",
    "#try:\n",
    "#    token_components = ACCESS_TOKEN.split(\".\")\n",
    "#    if len(token_components) == 3:\n",
    "#        header = json.loads(base64.b64decode(token_components[0]).decode())\n",
    "#        payload = json.loads(base64.b64decode(token_components[1]).decode())\n",
    "#        print(\"Token Header:\",json.dumps(header,indent=4))\n",
    "#        print(\"Token Payload:\",json.dumps(payload,indent=4))\n",
    "#        #print(\"Token Signature:\",token_components[2])\n",
    "#        print(\"Token Signature: <value_suppressed>\")\n",
    "#        #\n",
    "#        #  One should be able to use public/private keys to actually validate that signature - left as an exercise for later\n",
    "#        #\n",
    "#    else:\n",
    "#        print(f\"The ACCESS_TOKEN appears to be improperly structured. It should have 3 components and it has {len(token_components)}\")\n",
    "#except Exception as ex:\n",
    "#    print(f\"Looks like the ACCESS_TOKEN is undefined or an empty value\")\n",
    "#    raise(ex)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for getting article quality score:\n",
    "\n",
    "This section contains the code for a function named `request_ores_score_per_article`. The function is responsible for making API requests to gather article quality scores from the ORES service.\n",
    "\n",
    "#### Function Parameters:\n",
    "\n",
    "- `article_revid`: The revision ID of the article for which you want to retrieve the quality score.\n",
    "- `email_address`: Email address required for the API request.\n",
    "- `access_token`: Access token required for the API request.\n",
    "- `endpoint_url`: The API endpoint URL for making the request (default is `API_ORES_LIFTWING_ENDPOINT`).\n",
    "- `model_name`: The name of the ORES model for scoring (default is `API_ORES_EN_QUALITY_MODEL`).\n",
    "- `request_data`: Request data template for the API request (default is `ORES_REQUEST_DATA_TEMPLATE`).\n",
    "- `header_format`: Request header template for the API request (default is `REQUEST_HEADER_TEMPLATE`).\n",
    "- `header_params`: Request header parameters for the API request (default is `REQUEST_HEADER_PARAMS_TEMPLATE`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_ores_score_per_article(article_revid = None, \n",
    "                                   email_address=None,\n",
    "                                   access_token=None,\n",
    "                                   endpoint_url = API_ORES_LIFTWING_ENDPOINT, \n",
    "                                   model_name = API_ORES_EN_QUALITY_MODEL, \n",
    "                                   request_data = ORES_REQUEST_DATA_TEMPLATE, \n",
    "                                   header_format = REQUEST_HEADER_TEMPLATE, \n",
    "                                   header_params = REQUEST_HEADER_PARAMS_TEMPLATE):\n",
    "    \n",
    "    #    Make sure we have an article revision id, email and token\n",
    "    #    This approach prioritizes the parameters passed in when making the call\n",
    "    if article_revid:\n",
    "        request_data['rev_id'] = article_revid\n",
    "    if email_address:\n",
    "        header_params['email_address'] = email_address\n",
    "    if access_token:\n",
    "        header_params['access_token'] = access_token\n",
    "    \n",
    "    #   Making a request requires a revision id - an email address - and the access token\n",
    "    if not request_data['rev_id']:\n",
    "        raise Exception(\"Must provide an article revision id (rev_id) to score articles\")\n",
    "    if not header_params['email_address']:\n",
    "        raise Exception(\"Must provide an 'email_address' value\")\n",
    "    if not header_params['access_token']:\n",
    "        raise Exception(\"Must provide an 'access_token' value\")\n",
    "    \n",
    "    # Create the request URL with the specified model parameter - default is a article quality score request\n",
    "    request_url = endpoint_url.format(model_name=model_name)\n",
    "    \n",
    "    # Create a compliant request header from the template and the supplied parameters\n",
    "    headers = dict()\n",
    "    for key in header_format.keys():\n",
    "        headers[str(key)] = header_format[key].format(**header_params)\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free data\n",
    "        # source like ORES - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        #response = requests.get(request_url, headers=headers)\n",
    "        response = requests.post(request_url, headers=headers, data=json.dumps(request_data))\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection:\n",
    "This code block fetches quality scores for a set of articles using API requests to the ORES service. The retrieved scores are organized into a structured DataFrame for analysis and further processing.\n",
    "\n",
    "\n",
    "\n",
    "#### Initializing Data Structures:\n",
    "\n",
    "- An empty dictionary `quality_scores` is created to store the quality scores.\n",
    "- An empty list `failed_articles` is created to store articles for which scores couldn't be retrieved.\n",
    "\n",
    "#### Iteration Over DataFrame Rows:\n",
    "\n",
    "- It iterates over each row of a DataFrame named `result_df` that has the article tile and the revision ID.\n",
    "- For each row, it extracts the 'title' and 'lastrevid' values, which represent the article title and revision ID, respectively.\n",
    "\n",
    "#### API Requests for Quality Scores:\n",
    "\n",
    "- For each article, the code makes an API request to fetch the quality score.\n",
    "- The quality score is requested from the ORES API, and the response is stored in the `quality_score` variable.\n",
    "\n",
    "#### Data Storage:\n",
    "\n",
    "- If the quality score is successfully retrieved, it is stored in the `quality_scores` dictionary with the article title as the key.\n",
    "\n",
    "#### Handling Failed Requests:\n",
    "\n",
    "- If a request fails (i.e. returns {'httpCode': 429, 'httpReason': ''} or {}), the code logs the failed article titles in `failed_articles` and continues processing others.\n",
    "- The list of artciles without scores are displayed towards the end of the loop.\n",
    "\n",
    "#### Data Transformation:\n",
    "\n",
    "After processing all articles, the code aggregates the retrieved quality scores into a structured DataFrame.\n",
    "\n",
    "#### Output:\n",
    "\n",
    "The resulting DataFrame `quality_scores_df` contains information about quality scores for each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting quality score for:  Abbeville, Alabama , with rev ID:  1171163550\n",
      "{'enwiki': {'models': {'articlequality': {'version': '0.9.2'}}, 'scores': {'1171163550': {'articlequality': {'score': {'prediction': 'C', 'probability': {'B': 0.31042252456158204, 'C': 0.5979200965294227, 'FA': 0.025186220917133947, 'GA': 0.04952133645299354, 'Start': 0.013573873336789355, 'Stub': 0.0033759482020785892}}}}}}}\n",
      "Requesting quality score for:  Adamsville, Alabama , with rev ID:  1177621427\n",
      "{'enwiki': {'models': {'articlequality': {'version': '0.9.2'}}, 'scores': {'1177621427': {'articlequality': {'score': {'prediction': 'C', 'probability': {'B': 0.198274200391586, 'C': 0.3770695177348356, 'FA': 0.019070364455845708, 'GA': 0.3514876684327692, 'Start': 0.05026148902798659, 'Stub': 0.003836759956977147}}}}}}}\n",
      "Requesting quality score for:  Addison, Alabama , with rev ID:  1168359898\n",
      "{'enwiki': {'models': {'articlequality': {'version': '0.9.2'}}, 'scores': {'1168359898': {'articlequality': {'score': {'prediction': 'C', 'probability': {'B': 0.27104076563661905, 'C': 0.324459707767518, 'FA': 0.011265514086494389, 'GA': 0.29487067754320384, 'Start': 0.0931882446366844, 'Stub': 0.005175090329480344}}}}}}}\n",
      "Requesting quality score for:  Akron, Alabama , with rev ID:  1165909508\n",
      "{'enwiki': {'models': {'articlequality': {'version': '0.9.2'}}, 'scores': {'1165909508': {'articlequality': {'score': {'prediction': 'GA', 'probability': {'B': 0.175388344565975, 'C': 0.2655870765311225, 'FA': 0.011556876058535826, 'GA': 0.4485841879139288, 'Start': 0.09350806348909406, 'Stub': 0.005375451441343951}}}}}}}\n",
      "Requesting quality score for:  Alabaster, Alabama , with rev ID:  1179139816\n",
      "{'enwiki': {'models': {'articlequality': {'version': '0.9.2'}}, 'scores': {'1179139816': {'articlequality': {'score': {'prediction': 'C', 'probability': {'B': 0.270971932616856, 'C': 0.6463838191722866, 'FA': 0.009590992690925318, 'GA': 0.033641571757281816, 'Start': 0.036340860820955355, 'Stub': 0.003070822941694893}}}}}}}\n",
      "Requesting quality score for:  Albertville, Alabama , with rev ID:  1179198677\n",
      "{'enwiki': {'models': {'articlequality': {'version': '0.9.2'}}, 'scores': {'1179198677': {'articlequality': {'score': {'prediction': 'C', 'probability': {'B': 0.33263794328683494, 'C': 0.5751561883917391, 'FA': 0.026008649369378935, 'GA': 0.051684120237779414, 'Start': 0.011184137224920173, 'Stub': 0.0033289614893475604}}}}}}}\n",
      "Requesting quality score for:  Alexander City, Alabama , with rev ID:  1179140073\n",
      "{'enwiki': {'models': {'articlequality': {'version': '0.9.2'}}, 'scores': {'1179140073': {'articlequality': {'score': {'prediction': 'GA', 'probability': {'B': 0.1873023626039258, 'C': 0.36649319284269144, 'FA': 0.027896630393037452, 'GA': 0.38946719113684847, 'Start': 0.024584055969215264, 'Stub': 0.004256567054281696}}}}}}}\n",
      "Requesting quality score for:  Aliceville, Alabama , with rev ID:  1167792390\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-0ce5f01785d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Make a request to the ORES API to get the quality score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mquality_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequest_ores_score_per_article\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_revid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marticle_revid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memail_address\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"adithyaa279@gmail.com\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccess_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mACCESS_TOKEN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquality_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-d1bb3b4f2035>\u001b[0m in \u001b[0;36mrequest_ores_score_per_article\u001b[1;34m(article_revid, email_address, access_token, endpoint_url, model_name, request_data, header_format, header_params)\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAPI_THROTTLE_WAIT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;31m#response = requests.get(request_url, headers=headers)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mjson_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \"\"\"\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    540\u001b[0m         }\n\u001b[0;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    443\u001b[0m                     \u001b[1;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1345\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1347\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1348\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1239\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1240\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1241\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1242\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize an empty dictionary to store quality scores\n",
    "quality_scores = {}\n",
    "# Initialize an empty list to store articles for which scores couldn't be retrieved\n",
    "failed_articles = []\n",
    "\n",
    "# Iterate through the DataFrame with article titles and revision IDs\n",
    "for index, row in result_df.iterrows():\n",
    "    article_title = row['title']\n",
    "    article_revid = row['lastrevid']\n",
    "    print('Requesting quality score for: ', article_title,', with rev ID: ',article_revid)\n",
    "\n",
    "    # Make a request to the ORES API to get the quality score\n",
    "    quality_score = request_ores_score_per_article(article_revid=article_revid, email_address=\"adithyaa279@gmail.com\", access_token=ACCESS_TOKEN)\n",
    "    \n",
    "    # Check to collect articles with no scores\n",
    "    if 'httpCode' in quality_score:\n",
    "        print(f\"Failed to retrieve quality score for {article_title}\")\n",
    "        failed_articles.append(article_title)\n",
    "    else:\n",
    "        if quality_score is not None:\n",
    "            quality_scores[article_title] = quality_score\n",
    "        else:\n",
    "            print(f\"Failed to retrieve quality score for {article_title}\")\n",
    "            failed_articles.append(article_title)\n",
    "\n",
    "\n",
    "# Initialize lists to store page titles and quality scores as a dataframe\n",
    "page_titles = []\n",
    "revision_ids = []\n",
    "predictions = []\n",
    "probabilities_B = []\n",
    "probabilities_C = []\n",
    "probabilities_FA = []\n",
    "probabilities_GA = []\n",
    "probabilities_Start = []\n",
    "probabilities_Stub = []\n",
    "\n",
    "for page_title, page_data in quality_scores.items():\n",
    "    enwiki = page_data.get('enwiki', {})\n",
    "    if 'scores' in enwiki:\n",
    "        scores = enwiki['scores']\n",
    "        for rev_id, rev_data in scores.items():\n",
    "            prediction = rev_data['articlequality']['score']['prediction']\n",
    "            probability = rev_data['articlequality']['score']['probability']\n",
    "            page_titles.append(page_title)\n",
    "            revision_ids.append(rev_id)\n",
    "            predictions.append(prediction)\n",
    "            probabilities_B.append(probability['B'])\n",
    "            probabilities_C.append(probability['C'])\n",
    "            probabilities_FA.append(probability['FA'])\n",
    "            probabilities_GA.append(probability['GA'])\n",
    "            probabilities_Start.append(probability['Start'])\n",
    "            probabilities_Stub.append(probability['Stub'])\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "quality_scores_df = pd.DataFrame({\n",
    "    'page_title': page_titles,\n",
    "    'revision_id': revision_ids,\n",
    "    'prediction': predictions,\n",
    "    'probability_B': probabilities_B,\n",
    "    'probability_C': probabilities_C,\n",
    "    'probability_FA': probabilities_FA,\n",
    "    'probability_GA': probabilities_GA,\n",
    "    'probability_Start': probabilities_Start,\n",
    "    'probability_Stub': probabilities_Stub\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and save the list of failed articles\n",
    "print(\"Failed to retrieve ORES scores for the following articles:\")\n",
    "for article in failed_articles:\n",
    "    print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_title</th>\n",
       "      <th>revision_id</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probability_B</th>\n",
       "      <th>probability_C</th>\n",
       "      <th>probability_FA</th>\n",
       "      <th>probability_GA</th>\n",
       "      <th>probability_Start</th>\n",
       "      <th>probability_Stub</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>1171163550</td>\n",
       "      <td>C</td>\n",
       "      <td>0.310423</td>\n",
       "      <td>0.597920</td>\n",
       "      <td>0.025186</td>\n",
       "      <td>0.049521</td>\n",
       "      <td>0.013574</td>\n",
       "      <td>0.003376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>1177621427</td>\n",
       "      <td>C</td>\n",
       "      <td>0.198274</td>\n",
       "      <td>0.377070</td>\n",
       "      <td>0.019070</td>\n",
       "      <td>0.351488</td>\n",
       "      <td>0.050261</td>\n",
       "      <td>0.003837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>1168359898</td>\n",
       "      <td>C</td>\n",
       "      <td>0.271041</td>\n",
       "      <td>0.324460</td>\n",
       "      <td>0.011266</td>\n",
       "      <td>0.294871</td>\n",
       "      <td>0.093188</td>\n",
       "      <td>0.005175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>1165909508</td>\n",
       "      <td>GA</td>\n",
       "      <td>0.175388</td>\n",
       "      <td>0.265587</td>\n",
       "      <td>0.011557</td>\n",
       "      <td>0.448584</td>\n",
       "      <td>0.093508</td>\n",
       "      <td>0.005375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>1179139816</td>\n",
       "      <td>C</td>\n",
       "      <td>0.270972</td>\n",
       "      <td>0.646384</td>\n",
       "      <td>0.009591</td>\n",
       "      <td>0.033642</td>\n",
       "      <td>0.036341</td>\n",
       "      <td>0.003071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16673</th>\n",
       "      <td>Wamsutter, Wyoming</td>\n",
       "      <td>1169591845</td>\n",
       "      <td>GA</td>\n",
       "      <td>0.140348</td>\n",
       "      <td>0.187580</td>\n",
       "      <td>0.011517</td>\n",
       "      <td>0.624482</td>\n",
       "      <td>0.032711</td>\n",
       "      <td>0.003363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16674</th>\n",
       "      <td>Wheatland, Wyoming</td>\n",
       "      <td>1176370621</td>\n",
       "      <td>GA</td>\n",
       "      <td>0.245020</td>\n",
       "      <td>0.285966</td>\n",
       "      <td>0.034640</td>\n",
       "      <td>0.395906</td>\n",
       "      <td>0.033736</td>\n",
       "      <td>0.004732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16675</th>\n",
       "      <td>Worland, Wyoming</td>\n",
       "      <td>1166347917</td>\n",
       "      <td>GA</td>\n",
       "      <td>0.160382</td>\n",
       "      <td>0.238469</td>\n",
       "      <td>0.026243</td>\n",
       "      <td>0.546966</td>\n",
       "      <td>0.023868</td>\n",
       "      <td>0.004072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16676</th>\n",
       "      <td>Wright, Wyoming</td>\n",
       "      <td>1166334449</td>\n",
       "      <td>GA</td>\n",
       "      <td>0.165136</td>\n",
       "      <td>0.323542</td>\n",
       "      <td>0.009909</td>\n",
       "      <td>0.467899</td>\n",
       "      <td>0.029659</td>\n",
       "      <td>0.003855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16677</th>\n",
       "      <td>Yoder, Wyoming</td>\n",
       "      <td>1171182284</td>\n",
       "      <td>C</td>\n",
       "      <td>0.252816</td>\n",
       "      <td>0.340595</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.123967</td>\n",
       "      <td>0.260352</td>\n",
       "      <td>0.014970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16678 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                page_title revision_id prediction  probability_B  \\\n",
       "0       Abbeville, Alabama  1171163550          C       0.310423   \n",
       "1      Adamsville, Alabama  1177621427          C       0.198274   \n",
       "2         Addison, Alabama  1168359898          C       0.271041   \n",
       "3           Akron, Alabama  1165909508         GA       0.175388   \n",
       "4       Alabaster, Alabama  1179139816          C       0.270972   \n",
       "...                    ...         ...        ...            ...   \n",
       "16673   Wamsutter, Wyoming  1169591845         GA       0.140348   \n",
       "16674   Wheatland, Wyoming  1176370621         GA       0.245020   \n",
       "16675     Worland, Wyoming  1166347917         GA       0.160382   \n",
       "16676      Wright, Wyoming  1166334449         GA       0.165136   \n",
       "16677       Yoder, Wyoming  1171182284          C       0.252816   \n",
       "\n",
       "       probability_C  probability_FA  probability_GA  probability_Start  \\\n",
       "0           0.597920        0.025186        0.049521           0.013574   \n",
       "1           0.377070        0.019070        0.351488           0.050261   \n",
       "2           0.324460        0.011266        0.294871           0.093188   \n",
       "3           0.265587        0.011557        0.448584           0.093508   \n",
       "4           0.646384        0.009591        0.033642           0.036341   \n",
       "...              ...             ...             ...                ...   \n",
       "16673       0.187580        0.011517        0.624482           0.032711   \n",
       "16674       0.285966        0.034640        0.395906           0.033736   \n",
       "16675       0.238469        0.026243        0.546966           0.023868   \n",
       "16676       0.323542        0.009909        0.467899           0.029659   \n",
       "16677       0.340595        0.007300        0.123967           0.260352   \n",
       "\n",
       "       probability_Stub  \n",
       "0              0.003376  \n",
       "1              0.003837  \n",
       "2              0.005175  \n",
       "3              0.005375  \n",
       "4              0.003071  \n",
       "...                 ...  \n",
       "16673          0.003363  \n",
       "16674          0.004732  \n",
       "16675          0.004072  \n",
       "16676          0.003855  \n",
       "16677          0.014970  \n",
       "\n",
       "[16678 rows x 9 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export page info request as csv (intermediate file) \n",
    "quality_scores_df.to_csv(os.path.join(cwd, 'intermediate/scores.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Combining the Datasets\n",
    "This code block combines multiple datasets into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>regional_division</th>\n",
       "      <th>population</th>\n",
       "      <th>article_title</th>\n",
       "      <th>revision_id</th>\n",
       "      <th>article_quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>5074296.0</td>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>1171163550</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>5074296.0</td>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>1177621427</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>5074296.0</td>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>1168359898</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>5074296.0</td>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>1165909508</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>5074296.0</td>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>1179139816</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21520</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>581381.0</td>\n",
       "      <td>Wamsutter, Wyoming</td>\n",
       "      <td>1169591845</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21521</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>581381.0</td>\n",
       "      <td>Wheatland, Wyoming</td>\n",
       "      <td>1176370621</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21522</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>581381.0</td>\n",
       "      <td>Worland, Wyoming</td>\n",
       "      <td>1166347917</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21523</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>581381.0</td>\n",
       "      <td>Wright, Wyoming</td>\n",
       "      <td>1166334449</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21524</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>581381.0</td>\n",
       "      <td>Yoder, Wyoming</td>\n",
       "      <td>1171182284</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21525 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         state   regional_division  population        article_title  \\\n",
       "0      Alabama  East South Central   5074296.0   Abbeville, Alabama   \n",
       "1      Alabama  East South Central   5074296.0  Adamsville, Alabama   \n",
       "2      Alabama  East South Central   5074296.0     Addison, Alabama   \n",
       "3      Alabama  East South Central   5074296.0       Akron, Alabama   \n",
       "4      Alabama  East South Central   5074296.0   Alabaster, Alabama   \n",
       "...        ...                 ...         ...                  ...   \n",
       "21520  Wyoming            Mountain    581381.0   Wamsutter, Wyoming   \n",
       "21521  Wyoming            Mountain    581381.0   Wheatland, Wyoming   \n",
       "21522  Wyoming            Mountain    581381.0     Worland, Wyoming   \n",
       "21523  Wyoming            Mountain    581381.0      Wright, Wyoming   \n",
       "21524  Wyoming            Mountain    581381.0       Yoder, Wyoming   \n",
       "\n",
       "      revision_id article_quality  \n",
       "0      1171163550               C  \n",
       "1      1177621427               C  \n",
       "2      1168359898               C  \n",
       "3      1165909508              GA  \n",
       "4      1179139816               C  \n",
       "...           ...             ...  \n",
       "21520  1169591845              GA  \n",
       "21521  1176370621              GA  \n",
       "21522  1166347917              GA  \n",
       "21523  1166334449              GA  \n",
       "21524  1171182284               C  \n",
       "\n",
       "[21525 rows x 6 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a copy of the DataFrame to start the combining process\n",
    "combined_df = result_df.copy()\n",
    "\n",
    "# Merge with the DataFrame containing state information\n",
    "combined_df = combined_df.merge(us_citiesxstates, how='left',\n",
    "                                left_on='title', right_on='page_title')\n",
    "\n",
    "# Clean state names, handling any specific cases\n",
    "combined_df['state'] = combined_df['state'].apply(lambda x: 'Georgia' if x == 'Georgia_(U.S._state)' else x)\n",
    "\n",
    "# The cityxstate data had values like New_york, fixing that\n",
    "combined_df['state'] = combined_df.state.str.replace('_', ' ')\n",
    "\n",
    "# Convert lastrevid to string for proper merge\n",
    "combined_df['lastrevid'] = combined_df['lastrevid'].astype(str)\n",
    "\n",
    "# Merge with the DataFrame containing quality scores with revision ID\n",
    "combined_df = combined_df.merge(quality_scores_df[['revision_id', 'prediction']],\n",
    "                                left_on='lastrevid',\n",
    "                                right_on='revision_id',\n",
    "                                how='left')\n",
    "\n",
    "\n",
    "# Merge with the DataFrame containing regional division data\n",
    "combined_df = combined_df.merge(us_regions, how='left', on='state')\n",
    "\n",
    "# Merge with the DataFrame containing population data\n",
    "combined_df = combined_df.merge(us_pop[['State', '2022']], how='left', left_on='state', right_on='State')\n",
    "\n",
    "# Removing the duplicate revID column\n",
    "combined_df.drop('revision_id',inplace= True, axis =1)\n",
    "\n",
    "# Rename columns for consistency\n",
    "combined_df.rename(columns={'title': 'article_title',\n",
    "                            'lastrevid': 'revision_id',\n",
    "                            'prediction': 'article_quality',\n",
    "                            'division': 'regional_division',\n",
    "                            '2022': 'population'},\n",
    "                   inplace=True)\n",
    "\n",
    "# Select and reorder the relevant columns\n",
    "combined_df = combined_df[['state', 'regional_division', 'population',\n",
    "                           'article_title', 'revision_id', 'article_quality']]\n",
    "\n",
    "# Drop duplicate rows\n",
    "combined_df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "\n",
    "# Export the combined dataset to a CSV file\n",
    "combined_df.to_csv(os.path.join(cwd, 'output/wp_scored_city_articles_by_state.csv'),\n",
    "                   index=False)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Analysis\n",
    "\n",
    "This code block processes and analyzes data to calculate per capita ratios of articles and high-quality articles per state and regional division. It follows these key steps:\n",
    "\n",
    "#### Filtering High-Quality Articles:\n",
    "\n",
    "- High-quality articles (FA and GA classes) are selected from the dataset.\n",
    "\n",
    "#### Grouping Data:\n",
    "\n",
    "- The data is grouped by state and regional division.\n",
    "\n",
    "#### Calculating Article Statistics:\n",
    "\n",
    "- Total articles and high-quality articles are calculated for each state and division.\n",
    "\n",
    "#### Merging Population Data:\n",
    "\n",
    "- Population data is merged into the analysis data.\n",
    "\n",
    "#### Calculating Per Capita Ratios:\n",
    "\n",
    "- Per capita ratios for articles and high-quality articles are computed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>regional_division</th>\n",
       "      <th>total_articles</th>\n",
       "      <th>high_quality_articles</th>\n",
       "      <th>population</th>\n",
       "      <th>total_articles_per_capita</th>\n",
       "      <th>high_quality_articles_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>461</td>\n",
       "      <td>53.0</td>\n",
       "      <td>5074296.0</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>149</td>\n",
       "      <td>31.0</td>\n",
       "      <td>733583.0</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>91</td>\n",
       "      <td>24.0</td>\n",
       "      <td>7359197.0</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>West South Central</td>\n",
       "      <td>500</td>\n",
       "      <td>72.0</td>\n",
       "      <td>3045637.0</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>482</td>\n",
       "      <td>173.0</td>\n",
       "      <td>39029342.0</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>290</td>\n",
       "      <td>77.0</td>\n",
       "      <td>5839926.0</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Delaware</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>57</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1018396.0</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Florida</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>412</td>\n",
       "      <td>119.0</td>\n",
       "      <td>22244823.0</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Georgia</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>538</td>\n",
       "      <td>93.0</td>\n",
       "      <td>10912876.0</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hawaii</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>151</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1440196.0</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Idaho</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>201</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1939033.0</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Illinois</td>\n",
       "      <td>East North Central</td>\n",
       "      <td>1298</td>\n",
       "      <td>196.0</td>\n",
       "      <td>12582032.0</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Indiana</td>\n",
       "      <td>East North Central</td>\n",
       "      <td>565</td>\n",
       "      <td>124.0</td>\n",
       "      <td>6833037.0</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Iowa</td>\n",
       "      <td>West North Central</td>\n",
       "      <td>1043</td>\n",
       "      <td>104.0</td>\n",
       "      <td>3200517.0</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>West North Central</td>\n",
       "      <td>63</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2937150.0</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Kentucky</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>421</td>\n",
       "      <td>79.0</td>\n",
       "      <td>4512310.0</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Louisiana</td>\n",
       "      <td>West South Central</td>\n",
       "      <td>304</td>\n",
       "      <td>44.0</td>\n",
       "      <td>4590241.0</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Maine</td>\n",
       "      <td>New England</td>\n",
       "      <td>483</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1385340.0</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Maryland</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>157</td>\n",
       "      <td>42.0</td>\n",
       "      <td>6164660.0</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>New England</td>\n",
       "      <td>352</td>\n",
       "      <td>62.0</td>\n",
       "      <td>6981974.0</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Michigan</td>\n",
       "      <td>East North Central</td>\n",
       "      <td>1773</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10034113.0</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Minnesota</td>\n",
       "      <td>West North Central</td>\n",
       "      <td>854</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5717184.0</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mississippi</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2940057.0</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Missouri</td>\n",
       "      <td>West North Central</td>\n",
       "      <td>951</td>\n",
       "      <td>168.0</td>\n",
       "      <td>6177957.0</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Montana</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>128</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1122867.0</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>19</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3177772.0</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>New England</td>\n",
       "      <td>234</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1395231.0</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>New Jersey</td>\n",
       "      <td>Middle Atlantic</td>\n",
       "      <td>564</td>\n",
       "      <td>379.0</td>\n",
       "      <td>9261699.0</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>New Mexico</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>106</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2113344.0</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>New York</td>\n",
       "      <td>Middle Atlantic</td>\n",
       "      <td>661</td>\n",
       "      <td>111.0</td>\n",
       "      <td>19677151.0</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>North Carolina</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>50</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10698973.0</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>North Dakota</td>\n",
       "      <td>West North Central</td>\n",
       "      <td>356</td>\n",
       "      <td>26.0</td>\n",
       "      <td>779261.0</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Ohio</td>\n",
       "      <td>East North Central</td>\n",
       "      <td>926</td>\n",
       "      <td>204.0</td>\n",
       "      <td>11756058.0</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>West South Central</td>\n",
       "      <td>75</td>\n",
       "      <td>31.0</td>\n",
       "      <td>4019800.0</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Oregon</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>241</td>\n",
       "      <td>141.0</td>\n",
       "      <td>4240137.0</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>Middle Atlantic</td>\n",
       "      <td>2556</td>\n",
       "      <td>223.0</td>\n",
       "      <td>12972008.0</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Rhode Island</td>\n",
       "      <td>New England</td>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1093734.0</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>South Carolina</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>271</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5282634.0</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>South Dakota</td>\n",
       "      <td>West North Central</td>\n",
       "      <td>311</td>\n",
       "      <td>56.0</td>\n",
       "      <td>909824.0</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Tennessee</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>347</td>\n",
       "      <td>146.0</td>\n",
       "      <td>7051339.0</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Texas</td>\n",
       "      <td>West South Central</td>\n",
       "      <td>1224</td>\n",
       "      <td>487.0</td>\n",
       "      <td>30029572.0</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Utah</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>255</td>\n",
       "      <td>61.0</td>\n",
       "      <td>3380800.0</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Vermont</td>\n",
       "      <td>New England</td>\n",
       "      <td>329</td>\n",
       "      <td>45.0</td>\n",
       "      <td>647064.0</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>133</td>\n",
       "      <td>18.0</td>\n",
       "      <td>8683619.0</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Washington</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>281</td>\n",
       "      <td>115.0</td>\n",
       "      <td>7785786.0</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>West Virginia</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>232</td>\n",
       "      <td>106.0</td>\n",
       "      <td>1775156.0</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>East North Central</td>\n",
       "      <td>192</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5892539.0</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>99</td>\n",
       "      <td>39.0</td>\n",
       "      <td>581381.0</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             state   regional_division  total_articles  high_quality_articles  \\\n",
       "0          Alabama  East South Central             461                   53.0   \n",
       "1           Alaska             Pacific             149                   31.0   \n",
       "2          Arizona            Mountain              91                   24.0   \n",
       "3         Arkansas  West South Central             500                   72.0   \n",
       "4       California             Pacific             482                  173.0   \n",
       "5         Colorado            Mountain             290                   77.0   \n",
       "6         Delaware      South Atlantic              57                   25.0   \n",
       "7          Florida      South Atlantic             412                  119.0   \n",
       "8          Georgia      South Atlantic             538                   93.0   \n",
       "9           Hawaii             Pacific             151                   30.0   \n",
       "10           Idaho            Mountain             201                   41.0   \n",
       "11        Illinois  East North Central            1298                  196.0   \n",
       "12         Indiana  East North Central             565                  124.0   \n",
       "13            Iowa  West North Central            1043                  104.0   \n",
       "14          Kansas  West North Central              63                   22.0   \n",
       "15        Kentucky  East South Central             421                   79.0   \n",
       "16       Louisiana  West South Central             304                   44.0   \n",
       "17           Maine         New England             483                   43.0   \n",
       "18        Maryland      South Atlantic             157                   42.0   \n",
       "19   Massachusetts         New England             352                   62.0   \n",
       "20        Michigan  East North Central            1773                   12.0   \n",
       "21       Minnesota  West North Central             854                    NaN   \n",
       "22     Mississippi  East South Central             300                    NaN   \n",
       "23        Missouri  West North Central             951                  168.0   \n",
       "24         Montana            Mountain             128                   55.0   \n",
       "25          Nevada            Mountain              19                    8.0   \n",
       "26   New Hampshire         New England             234                   63.0   \n",
       "27      New Jersey     Middle Atlantic             564                  379.0   \n",
       "28      New Mexico            Mountain             106                   31.0   \n",
       "29        New York     Middle Atlantic             661                  111.0   \n",
       "30  North Carolina      South Atlantic              50                   21.0   \n",
       "31    North Dakota  West North Central             356                   26.0   \n",
       "32            Ohio  East North Central             926                  204.0   \n",
       "33        Oklahoma  West South Central              75                   31.0   \n",
       "34          Oregon             Pacific             241                  141.0   \n",
       "35    Pennsylvania     Middle Atlantic            2556                  223.0   \n",
       "36    Rhode Island         New England              39                    NaN   \n",
       "37  South Carolina      South Atlantic             271                   60.0   \n",
       "38    South Dakota  West North Central             311                   56.0   \n",
       "39       Tennessee  East South Central             347                  146.0   \n",
       "40           Texas  West South Central            1224                  487.0   \n",
       "41            Utah            Mountain             255                   61.0   \n",
       "42         Vermont         New England             329                   45.0   \n",
       "43        Virginia      South Atlantic             133                   18.0   \n",
       "44      Washington             Pacific             281                  115.0   \n",
       "45   West Virginia      South Atlantic             232                  106.0   \n",
       "46       Wisconsin  East North Central             192                   60.0   \n",
       "47         Wyoming            Mountain              99                   39.0   \n",
       "\n",
       "    population  total_articles_per_capita  high_quality_articles_per_capita  \n",
       "0    5074296.0                   0.000091                          0.000010  \n",
       "1     733583.0                   0.000203                          0.000042  \n",
       "2    7359197.0                   0.000012                          0.000003  \n",
       "3    3045637.0                   0.000164                          0.000024  \n",
       "4   39029342.0                   0.000012                          0.000004  \n",
       "5    5839926.0                   0.000050                          0.000013  \n",
       "6    1018396.0                   0.000056                          0.000025  \n",
       "7   22244823.0                   0.000019                          0.000005  \n",
       "8   10912876.0                   0.000049                          0.000009  \n",
       "9    1440196.0                   0.000105                          0.000021  \n",
       "10   1939033.0                   0.000104                          0.000021  \n",
       "11  12582032.0                   0.000103                          0.000016  \n",
       "12   6833037.0                   0.000083                          0.000018  \n",
       "13   3200517.0                   0.000326                          0.000032  \n",
       "14   2937150.0                   0.000021                          0.000007  \n",
       "15   4512310.0                   0.000093                          0.000018  \n",
       "16   4590241.0                   0.000066                          0.000010  \n",
       "17   1385340.0                   0.000349                          0.000031  \n",
       "18   6164660.0                   0.000025                          0.000007  \n",
       "19   6981974.0                   0.000050                          0.000009  \n",
       "20  10034113.0                   0.000177                          0.000001  \n",
       "21   5717184.0                   0.000149                               NaN  \n",
       "22   2940057.0                   0.000102                               NaN  \n",
       "23   6177957.0                   0.000154                          0.000027  \n",
       "24   1122867.0                   0.000114                          0.000049  \n",
       "25   3177772.0                   0.000006                          0.000003  \n",
       "26   1395231.0                   0.000168                          0.000045  \n",
       "27   9261699.0                   0.000061                          0.000041  \n",
       "28   2113344.0                   0.000050                          0.000015  \n",
       "29  19677151.0                   0.000034                          0.000006  \n",
       "30  10698973.0                   0.000005                          0.000002  \n",
       "31    779261.0                   0.000457                          0.000033  \n",
       "32  11756058.0                   0.000079                          0.000017  \n",
       "33   4019800.0                   0.000019                          0.000008  \n",
       "34   4240137.0                   0.000057                          0.000033  \n",
       "35  12972008.0                   0.000197                          0.000017  \n",
       "36   1093734.0                   0.000036                               NaN  \n",
       "37   5282634.0                   0.000051                          0.000011  \n",
       "38    909824.0                   0.000342                          0.000062  \n",
       "39   7051339.0                   0.000049                          0.000021  \n",
       "40  30029572.0                   0.000041                          0.000016  \n",
       "41   3380800.0                   0.000075                          0.000018  \n",
       "42    647064.0                   0.000508                          0.000070  \n",
       "43   8683619.0                   0.000015                          0.000002  \n",
       "44   7785786.0                   0.000036                          0.000015  \n",
       "45   1775156.0                   0.000131                          0.000060  \n",
       "46   5892539.0                   0.000033                          0.000010  \n",
       "47    581381.0                   0.000170                          0.000067  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter for high-quality articles (FA and GA)\n",
    "high_quality_articles = combined_df[combined_df['article_quality'].isin(['FA', 'GA'])]\n",
    "\n",
    "# Group the data by state and division\n",
    "grouped_data = combined_df.groupby(['state', 'regional_division'])\n",
    "\n",
    "# Calculate the total articles and high-quality articles\n",
    "total_articles = grouped_data['article_title'].count()\n",
    "high_quality_articles_count = high_quality_articles.groupby(['state', 'regional_division'])['article_title'].count()\n",
    "\n",
    "# Merge the population data\n",
    "analysis_data = grouped_data[['state', 'regional_division']].first()\n",
    "analysis_data['total_articles'] = total_articles\n",
    "analysis_data['high_quality_articles'] = high_quality_articles_count\n",
    "\n",
    "analysis_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Merge population data using 'state' column\n",
    "analysis_data = analysis_data.merge(us_pop, left_on='state', right_on='State', how='left')\n",
    "\n",
    "# Calculate per capita ratios\n",
    "analysis_data['total_articles_per_capita'] = analysis_data['total_articles'] / analysis_data['2022']\n",
    "analysis_data['high_quality_articles_per_capita'] = analysis_data['high_quality_articles'] / analysis_data['2022']\n",
    "\n",
    "# Drop repeated columns\n",
    "analysis_data.drop('State',inplace=True,axis=1)\n",
    "\n",
    "# Rename columns for consistency\n",
    "analysis_data.rename(columns={'2022': 'population'},\n",
    "                   inplace=True)\n",
    "\n",
    "analysis_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Results\n",
    "\n",
    "1.\tTop 10 US states by coverage: The 10 US states with the highest total articles per capita (in descending order) .\n",
    "2.\tBottom 10 US states by coverage: The 10 US states with the lowest total articles per capita (in ascending order) .\n",
    "3.\tTop 10 US states by high quality: The 10 US states with the highest high quality articles per capita (in descending order) .\n",
    "4.\tBottom 10 US states by high quality: The 10 US states with the lowest high quality articles per capita (in ascending order).\n",
    "5.\tCensus divisions by total coverage: A rank ordered list of US census divisions (in descending order) by total articles per capita.\n",
    "6.\tCensus divisions by high quality coverage: Rank ordered list of US census divisions (in descending order) by high quality articles per capita.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>regional_division</th>\n",
       "      <th>total_articles</th>\n",
       "      <th>population</th>\n",
       "      <th>total_articles_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vermont</td>\n",
       "      <td>New England</td>\n",
       "      <td>329</td>\n",
       "      <td>647064.0</td>\n",
       "      <td>0.000508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>North Dakota</td>\n",
       "      <td>West North Central</td>\n",
       "      <td>356</td>\n",
       "      <td>779261.0</td>\n",
       "      <td>0.000457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Maine</td>\n",
       "      <td>New England</td>\n",
       "      <td>483</td>\n",
       "      <td>1385340.0</td>\n",
       "      <td>0.000349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>South Dakota</td>\n",
       "      <td>West North Central</td>\n",
       "      <td>311</td>\n",
       "      <td>909824.0</td>\n",
       "      <td>0.000342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iowa</td>\n",
       "      <td>West North Central</td>\n",
       "      <td>1043</td>\n",
       "      <td>3200517.0</td>\n",
       "      <td>0.000326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>149</td>\n",
       "      <td>733583.0</td>\n",
       "      <td>0.000203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>Middle Atlantic</td>\n",
       "      <td>2556</td>\n",
       "      <td>12972008.0</td>\n",
       "      <td>0.000197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Michigan</td>\n",
       "      <td>East North Central</td>\n",
       "      <td>1773</td>\n",
       "      <td>10034113.0</td>\n",
       "      <td>0.000177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>99</td>\n",
       "      <td>581381.0</td>\n",
       "      <td>0.000170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>New England</td>\n",
       "      <td>234</td>\n",
       "      <td>1395231.0</td>\n",
       "      <td>0.000168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           state   regional_division  total_articles  population  \\\n",
       "0        Vermont         New England             329    647064.0   \n",
       "1   North Dakota  West North Central             356    779261.0   \n",
       "2          Maine         New England             483   1385340.0   \n",
       "3   South Dakota  West North Central             311    909824.0   \n",
       "4           Iowa  West North Central            1043   3200517.0   \n",
       "5         Alaska             Pacific             149    733583.0   \n",
       "6   Pennsylvania     Middle Atlantic            2556  12972008.0   \n",
       "7       Michigan  East North Central            1773  10034113.0   \n",
       "8        Wyoming            Mountain              99    581381.0   \n",
       "9  New Hampshire         New England             234   1395231.0   \n",
       "\n",
       "   total_articles_per_capita  \n",
       "0                   0.000508  \n",
       "1                   0.000457  \n",
       "2                   0.000349  \n",
       "3                   0.000342  \n",
       "4                   0.000326  \n",
       "5                   0.000203  \n",
       "6                   0.000197  \n",
       "7                   0.000177  \n",
       "8                   0.000170  \n",
       "9                   0.000168  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "top_10_states_by_coverage = analysis_data.sort_values(by='total_articles_per_capita', ascending=False).head(10)\n",
    "top_10_states_by_coverage.drop(['high_quality_articles','high_quality_articles_per_capita'],inplace=True,axis=1)\n",
    "top_10_states_by_coverage.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>regional_division</th>\n",
       "      <th>total_articles</th>\n",
       "      <th>population</th>\n",
       "      <th>total_articles_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>North Carolina</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>50</td>\n",
       "      <td>10698973.0</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>19</td>\n",
       "      <td>3177772.0</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>California</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>482</td>\n",
       "      <td>39029342.0</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>91</td>\n",
       "      <td>7359197.0</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>133</td>\n",
       "      <td>8683619.0</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Florida</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>412</td>\n",
       "      <td>22244823.0</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>West South Central</td>\n",
       "      <td>75</td>\n",
       "      <td>4019800.0</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>West North Central</td>\n",
       "      <td>63</td>\n",
       "      <td>2937150.0</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Maryland</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>157</td>\n",
       "      <td>6164660.0</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>East North Central</td>\n",
       "      <td>192</td>\n",
       "      <td>5892539.0</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            state   regional_division  total_articles  population  \\\n",
       "0  North Carolina      South Atlantic              50  10698973.0   \n",
       "1          Nevada            Mountain              19   3177772.0   \n",
       "2      California             Pacific             482  39029342.0   \n",
       "3         Arizona            Mountain              91   7359197.0   \n",
       "4        Virginia      South Atlantic             133   8683619.0   \n",
       "5         Florida      South Atlantic             412  22244823.0   \n",
       "6        Oklahoma  West South Central              75   4019800.0   \n",
       "7          Kansas  West North Central              63   2937150.0   \n",
       "8        Maryland      South Atlantic             157   6164660.0   \n",
       "9       Wisconsin  East North Central             192   5892539.0   \n",
       "\n",
       "   total_articles_per_capita  \n",
       "0                   0.000005  \n",
       "1                   0.000006  \n",
       "2                   0.000012  \n",
       "3                   0.000012  \n",
       "4                   0.000015  \n",
       "5                   0.000019  \n",
       "6                   0.000019  \n",
       "7                   0.000021  \n",
       "8                   0.000025  \n",
       "9                   0.000033  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2\n",
    "bottom_10_states_by_coverage = analysis_data.sort_values(by='total_articles_per_capita').head(10)\n",
    "bottom_10_states_by_coverage.drop(['high_quality_articles','high_quality_articles_per_capita'],inplace=True,axis=1)\n",
    "bottom_10_states_by_coverage.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>regional_division</th>\n",
       "      <th>high_quality_articles</th>\n",
       "      <th>population</th>\n",
       "      <th>high_quality_articles_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vermont</td>\n",
       "      <td>New England</td>\n",
       "      <td>45.0</td>\n",
       "      <td>647064.0</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>39.0</td>\n",
       "      <td>581381.0</td>\n",
       "      <td>0.000067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>South Dakota</td>\n",
       "      <td>West North Central</td>\n",
       "      <td>56.0</td>\n",
       "      <td>909824.0</td>\n",
       "      <td>0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>West Virginia</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>106.0</td>\n",
       "      <td>1775156.0</td>\n",
       "      <td>0.000060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Montana</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1122867.0</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>New England</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1395231.0</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>31.0</td>\n",
       "      <td>733583.0</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>New Jersey</td>\n",
       "      <td>Middle Atlantic</td>\n",
       "      <td>379.0</td>\n",
       "      <td>9261699.0</td>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>North Dakota</td>\n",
       "      <td>West North Central</td>\n",
       "      <td>26.0</td>\n",
       "      <td>779261.0</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Oregon</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>141.0</td>\n",
       "      <td>4240137.0</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           state   regional_division  high_quality_articles  population  \\\n",
       "0        Vermont         New England                   45.0    647064.0   \n",
       "1        Wyoming            Mountain                   39.0    581381.0   \n",
       "2   South Dakota  West North Central                   56.0    909824.0   \n",
       "3  West Virginia      South Atlantic                  106.0   1775156.0   \n",
       "4        Montana            Mountain                   55.0   1122867.0   \n",
       "5  New Hampshire         New England                   63.0   1395231.0   \n",
       "6         Alaska             Pacific                   31.0    733583.0   \n",
       "7     New Jersey     Middle Atlantic                  379.0   9261699.0   \n",
       "8   North Dakota  West North Central                   26.0    779261.0   \n",
       "9         Oregon             Pacific                  141.0   4240137.0   \n",
       "\n",
       "   high_quality_articles_per_capita  \n",
       "0                          0.000070  \n",
       "1                          0.000067  \n",
       "2                          0.000062  \n",
       "3                          0.000060  \n",
       "4                          0.000049  \n",
       "5                          0.000045  \n",
       "6                          0.000042  \n",
       "7                          0.000041  \n",
       "8                          0.000033  \n",
       "9                          0.000033  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3\n",
    "top_10_states_by_high_quality = analysis_data.sort_values(by='high_quality_articles_per_capita', ascending=False).head(10)\n",
    "top_10_states_by_high_quality.drop(['total_articles','total_articles_per_capita'],inplace=True,axis=1)\n",
    "top_10_states_by_high_quality.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>regional_division</th>\n",
       "      <th>high_quality_articles</th>\n",
       "      <th>population</th>\n",
       "      <th>high_quality_articles_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Michigan</td>\n",
       "      <td>East North Central</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10034113.0</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>North Carolina</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10698973.0</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>18.0</td>\n",
       "      <td>8683619.0</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3177772.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>24.0</td>\n",
       "      <td>7359197.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>California</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>173.0</td>\n",
       "      <td>39029342.0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Florida</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>119.0</td>\n",
       "      <td>22244823.0</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>New York</td>\n",
       "      <td>Middle Atlantic</td>\n",
       "      <td>111.0</td>\n",
       "      <td>19677151.0</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Maryland</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>42.0</td>\n",
       "      <td>6164660.0</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>West North Central</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2937150.0</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            state   regional_division  high_quality_articles  population  \\\n",
       "0        Michigan  East North Central                   12.0  10034113.0   \n",
       "1  North Carolina      South Atlantic                   21.0  10698973.0   \n",
       "2        Virginia      South Atlantic                   18.0   8683619.0   \n",
       "3          Nevada            Mountain                    8.0   3177772.0   \n",
       "4         Arizona            Mountain                   24.0   7359197.0   \n",
       "5      California             Pacific                  173.0  39029342.0   \n",
       "6         Florida      South Atlantic                  119.0  22244823.0   \n",
       "7        New York     Middle Atlantic                  111.0  19677151.0   \n",
       "8        Maryland      South Atlantic                   42.0   6164660.0   \n",
       "9          Kansas  West North Central                   22.0   2937150.0   \n",
       "\n",
       "   high_quality_articles_per_capita  \n",
       "0                          0.000001  \n",
       "1                          0.000002  \n",
       "2                          0.000002  \n",
       "3                          0.000003  \n",
       "4                          0.000003  \n",
       "5                          0.000004  \n",
       "6                          0.000005  \n",
       "7                          0.000006  \n",
       "8                          0.000007  \n",
       "9                          0.000007  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4\n",
    "bottom_10_states_by_high_quality = analysis_data.sort_values(by='high_quality_articles_per_capita').head(10)\n",
    "bottom_10_states_by_high_quality.drop(['total_articles','total_articles_per_capita'],inplace=True,axis=1)\n",
    "bottom_10_states_by_high_quality.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regional_division</th>\n",
       "      <th>total_articles_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>West North Central</td>\n",
       "      <td>0.000242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New England</td>\n",
       "      <td>0.000222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Middle Atlantic</td>\n",
       "      <td>0.000097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>East North Central</td>\n",
       "      <td>0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>East South Central</td>\n",
       "      <td>0.000084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>0.000083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>0.000073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>West South Central</td>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>0.000044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    regional_division  total_articles_per_capita\n",
       "0  West North Central                   0.000242\n",
       "1         New England                   0.000222\n",
       "2     Middle Atlantic                   0.000097\n",
       "3  East North Central                   0.000095\n",
       "4  East South Central                   0.000084\n",
       "5             Pacific                   0.000083\n",
       "6            Mountain                   0.000073\n",
       "7  West South Central                   0.000072\n",
       "8      South Atlantic                   0.000044"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5\n",
    "census_divisions_by_total_coverage = analysis_data.groupby('regional_division').agg({'total_articles_per_capita': 'mean'}).reset_index()\n",
    "census_divisions_by_total_coverage = census_divisions_by_total_coverage.sort_values(by='total_articles_per_capita', ascending=False)\n",
    "census_divisions_by_total_coverage.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regional_division</th>\n",
       "      <th>high_quality_articles_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New England</td>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>West North Central</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mountain</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>0.000023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Middle Atlantic</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>East South Central</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>West South Central</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>East North Central</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    regional_division  high_quality_articles_per_capita\n",
       "0         New England                          0.000039\n",
       "1  West North Central                          0.000032\n",
       "2            Mountain                          0.000024\n",
       "3             Pacific                          0.000023\n",
       "4     Middle Atlantic                          0.000021\n",
       "5  East South Central                          0.000016\n",
       "6      South Atlantic                          0.000015\n",
       "7  West South Central                          0.000014\n",
       "8  East North Central                          0.000012"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6\n",
    "census_divisions_by_high_quality_coverage = analysis_data.groupby('regional_division').agg({'high_quality_articles_per_capita': 'mean'}).reset_index()\n",
    "census_divisions_by_high_quality_coverage = census_divisions_by_high_quality_coverage.sort_values(by='high_quality_articles_per_capita', ascending=False)\n",
    "census_divisions_by_high_quality_coverage.reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
